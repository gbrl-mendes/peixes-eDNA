---
title: "eDNA-LGC Metagenomics Pipeline"
author: "Hilário, OH; Mendes, GA"
date: "22/03/2022"
output:
  html_document:  
    code_download: yes
    theme: flatly
    toc: true
    toc_depth: 4
    toc_float: true
  pdf_document: default
editor_options:
  chunk_output_type: console
  markdown: 
    wrap: 72
---

------------------------------------------------------------------------

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load myData, include=FALSE}
load(file = "/home/gabriel/projetos/peixes-eDNA/analises/li_22mar22/li_22mar22-env_2022-03-23_blast.RData")
```

## Introdução

Este *pipeline* foi desenvolvido pelo Dr. Heron Hilário e comentado e
adaptado por Gabriel Antônio Mendes. Seu desenvolvimento ocorreu sob
orientação do Professor Dr. Daniel de Cardoso Carvalho no Laboratório de
Genética da Conservação da Pontífice Universidade Católica de Minas
Gerais, Brasil (LGC PUC-MG).

Este código possui como objetivo principal realizar a identificação
taxonômica de amplicons gerados a partir do material genético extraído
de amostras ambientais, como água ou solo. Ele foi desenvolvido tendo em
seu núcleo o uso do pacote dada2, que utiliza do algorítmo DADA2 para
inferir as sequências exatas das variantes de amplicons (ASVs), a partir
de dados HTS *(High Throughput Sequencig)* gerados por sequenciamentos
Illumina.

Para a realização da inferência taxonômica das ASVs utilizamos de
referência um banco de dados próprio, o db12SLGC, onde estão depositadas
as sequências do gene rRNA mitocondrial 12S das espécies de peixes das
bacias dos Rio Jequitinhonha e São Francisco. O DADA2 pode não ser
suficiente para realizar a atribuição taxonômica de todos os amplicons
gerados, e por isso também foi implementado no *pipeline* a
identificação a partir do BLAST, realizada simultaneamente.

Atualmente, sabe-se que uma abordagem que utilize apenas a inferência da
sequência exata das ASVs não é suficiente para realizar a identificação
adequada. Por isso, o *eDNA-LGC Metagenomics Pipeline* utiliza também o
SWARM para a realização da inferência de OTUs.

Se você tem alguma dúvida ou sugestão, por favor sinta-se à vontade para
escrever um e-mail para
[lgc.edna\@gmail.com](mailto:lgc.edna@gmail.com){.email}

## Preparação

Carregando bibliotecas do *R* e programas do sistema.

```{r, eval=FALSE,echo=TRUE}

## Carregando o arquivo de environment do projeto e Rmd e os pacotes necessários

# Carregando pacotes ----
{
  library(dplyr)
  library(tidyr)
  library(tibble)
  library(stringr)
  library(ggplot2)
  library(phyloseq)
  library(Biostrings)
  library(ShortRead)
  library(dada2)
  library(DECIPHER)
  library(future)
}

## Definindo o caminho para o executável do cutadapt

cutadapt <- "/usr/local/bin/cutadapt"


# Definindo o caminho das pastas essenciais para o projeto:

# 1 - Definindo o caminhos de saida e de entrada ----

# usar o mesmo caminho que foi criado na variavel do bash $PRJCT_DIR
analysis_path <- "/home/gabriel/projetos/peixes-eDNA/analises/teste_08abr22"

# criacao do data_folder
data_path <- paste0(analysis_path,"/data")
if(!dir.exists(data_path)) dir.create(data_path)

# criacao do diretorio das reads processadas
pipe_libs <- paste0(data_path,"/reads")
if(!dir.exists(pipe_libs)) dir.create(pipe_libs)

# criacao da pasta de resultados
results_path <- paste0(analysis_path,"/results")
if(!dir.exists(results_path)) dir.create(results_path)

# criacao da pasta de figuras
figs_path <- paste0(results_path,"/figs")
if(!dir.exists(figs_path)) dir.create(figs_path)

# criacao da pasta do swarm
swarm_path <- paste0(results_path,"/swarm")
if(!dir.exists(swarm_path)) dir.create(swarm_path)

#criacao do radical do nome do projeto
prjct_rad <-c("teste_08abr22")

#garantir que as pastas estao vazias
list.files(analysis_path)

# 2 - Indicar onde as reads brutas estao ----
run_path <- "/home/gabriel/projetos/peixes-eDNA/raw"


## Todas as bibliotecas ja estao demultiplexadas
raw_libs <- "/home/gabriel/projetos/peixes-eDNA/raw" # PATH to the directory containing raw fastq files (same as in $RAW_DATA).

#funcao para conferir se o diretorio salvo em raw_libs realmente contem os arquivos que voce quer analisar
list.files(path = raw_libs)


```

## Obtenção dos dados

Download das amostras demultiplexadas do *Illumina BaseSpace Sequence
Hub* utilizando a interface *bs*.

```{bash, eval=FALSE}

# navegando ate a pasta com os arquivos brutos
cd $raw_data_folder/$run_folder;

#realizando a autenticacao no BaseSpace Sequence Hub (apenas no primeiro log in)
bs auth;

#exibindo os sequenciamentos das corridas no seu repositório BaseSpace
bs list datasets;


bs download project -n "fish_metabarcoding_dez21" -o fastq --extension=fastq.gz;

# 3 - organize raw data
# store raw data path into variable
RAW_DATA=/home/heron/runs/run5_22dez21/raw
#create folder for raw files
mkdir $RAW_DATA;
#copy/move all fastqs to raw folder
mv /home/heron/runs/run5_22dez21/fastq/*/*fastq.gz $RAW_DATA; 
```

## Demultiplexando as amostras

-   Cada biblioteca possui amostras que estão combinadas sob um mesmo
    índex Illumina. Agora iremos utilizar as sequências dos primers
    indexados para selecionais quais reads correspondem a cada amostra
    única. Já que o kit de preparação de bibliotecas utilizado (Collibri
    -ThermoFisher) resulta em amplicons sequenciados em ambas
    orientações, iremos partir do princípio que é possível encontrar os
    primers FWD e REV em qualquer conjunto de reads. Para que as
    próximas etapas ocorram adequadamente, deveremos fornecer um arquivo
    .csv correspondente aos metadados associados a todas amostras.

-   Esta tabela deve obrigatoriamente conter três colunas, *File_name*,
    *Tag FWD*, *Tag REV*. Cada *amostra* deve corresponder a um valor
    único em *File_name* e a uma combinação única de *Tag FWD* e *Tag
    REV*.

### Regenerando index+primer

```{r, eval=FALSE,echo=TRUE}
# carregar os indexes dos primers e a tabela de amostras 

# Esta é a entrada mais importante da análise 
# Este input e o arquivo .sv com colunas com valores unicos: File_name, (combined) "Tag FWD-Tag REV"

primers_n_samples <- readr::read_csv(file = "/home/gabriel/projetos/peixes-eDNA/analises/teste_08abr22/data/LI_primers_n_samples.csv")


# conferir se há algum nome de arquivo duplicado ----
primers_n_samples$File_name %>% length()
primers_n_samples$File_name %>% unique() %>% length()
primers_n_samples$File_name %>% duplicated() %>% which()
primers_n_samples$File_name[primers_n_samples$File_name %>% duplicated()]

# conferir se há alguma combinação de tags repetida ----
select(primers_n_samples,c("Tag FWD","Tag REV")) %>% unite(col = "col",sep = "-") %>% nrow()
select(primers_n_samples,c("Tag FWD","Tag REV")) %>% unite(col = "col",sep = "-") %>% unique() %>% nrow()
select(primers_n_samples,c("Tag FWD","Tag REV")) %>% unite(col = "col",sep = "-") %>% duplicated() %>% which()
select(primers_n_samples,c("Tag FWD","Tag REV")) %>% unite(col = "col",sep = "-") %>% duplicated() %>% which()
primers_n_samples$File_name[select(primers_n_samples,c("Tag FWD","Tag REV")) %>% unite(col = "col",sep = "-") %>% duplicated() %>% which()]

#carregar sequências de primers sem Ns ----
primers_seqs <-  Biostrings::readDNAStringSet(filepath = "/home/gabriel/projetos/peixes-eDNA/analises/teste_08abr22/data/neo_miU_indexed_primers.fasta",format = "fasta")

primers_seqs <- primers_seqs %>% 
  as.data.frame() %>% 
  `colnames<-`("Primer seq") %>% 
  mutate(`Primer name` = rownames(.)) %>% 
  as_tibble()

```

### Demultiplexando com cutadapt

<https://cutadapt.readthedocs.io/en/stable/guide.html#combinatorial-demultiplexing>

```{bash, eval=FALSE}
#na edna com
ulimit -n 1000000

#combine file to demultiplex at once
 cat $RAW_DATA/*22dez21*R1* > $RAW_DATA/LGC_run5_R1.fastq.gz
 cat $RAW_DATA/*22dez21*R2* > $RAW_DATA/LGC_run5_R2.fastq.gz


cutadapt -e 0.10 -j 79 --no-indels --max-n 0\
 -g file:/home/gabriel/projetos/peixes-eDNA/analyses/LGC_run5_22dez21/data/neo_miU_indexed_primers.fasta  \
 -G file:/home/gabriel/projetos/peixes-eDNA/analyses/LGC_run5_22dez21/data/neo_miU_indexed_primers.fasta \
 -o /home/heron/runs/run5_22dez21/dmux_CDI/{name1}-{name2}.R1.fastq \
 -p /home/heron/runs/run5_22dez21/dmux_CDI/{name1}-{name2}.R2.fastq \
 /home/heron/runs/run5_22dez21/raw/LGC_run5_R1.fastq.gz \
 /home/heron/runs/run5_22dez21/raw/LGC_run5_R2.fastq.gz \
 2> /home/heron/runs/run5_22dez21/eDNA_LGC_run5_cut_010_noNs.txt;
 
 #ls -lahSr /home/heron/runs/run5_22dez21/dmux_CDI/| grep -v " 0 Oct"
 ls -lahSr /home/heron/runs/run5_22dez21/dmux_CDI/| grep -v ;

#remove empty files (don't, it will generate conflicts soon)
find ~/runs/run_28out21/dmux_CDI$ \
 -size 0 \
 -delete;


```

## Preparação dos arquivos brutos para o DADA2

### Preparando os arquivos

```{r, eval=FALSE,echo=TRUE}

## Criação do objeto libs_path com o caminho das bibliotecas demultiplexadas na eDNA: 
libs_path <- "/home/gabriel/projetos/peixes-eDNA/raw"
  

all_fnFs <- sort(list.files(libs_path, pattern=".R1.fastq", full.names = TRUE))
all_fnRs <- sort(list.files(libs_path, pattern=".R2.fastq", full.names = TRUE))

## Confira se ambos os objetos possuem tamanhos idênticos:
length(all_fnFs) 
length(all_fnRs)

carregando os dados das amostras (origem e indexes)

colnames(primers_n_samples)

sample_idx_tbl <- primers_n_samples %>% select(c("File_name", "Tag FWD", "Tag REV"))

# Verificar se existem amostras com nomes repetidos:
sample_idx_tbl$File_name %>% unique()
sample_idx_tbl$File_name[sample_idx_tbl$File_name %>% duplicated()]
# Não devem haver amostras com nomes iguais!! 
# Caso existam, editar na tabela primers_n_samples!


# Criação das colunas Lib name F a partir dos indexes FWD e REV de cada amostra, contida em primers_n_samples:
sample_idx_tbl <- sample_idx_tbl %>%
  unite(col = `Lib name F`,c(`Tag FWD`,`Tag REV`), sep = "-", remove = FALSE) %>%
  unite(col = `Lib name R`,c(`Tag REV`,`Tag FWD`), sep = "-", remove = FALSE)

# Adicionando na tabela sample_idx_table as colunas FWD_R1, FWD_R2, REV_R1 e REV_R2:
sample_idx_tbl <- sample_idx_tbl %>%
  mutate("FWD_R1" = "F-R1",
         "FWD_R2" = "F-R2",
         "REV_R1" = "R-R1",
         "REV_R2" = "R-R2")

# Substituindo o conteúdo das colunas FWD_R1, FWD_R2, REV_R1 e REV_R2 pelo endereço de cada amostra na pasta peixes-eDNA/raw
for (sample in 1:nrow(sample_idx_tbl)) {

  sample_idx_tbl$FWD_R1[sample] <-
    all_fnFs[grep(pattern =  paste0("/",sample_idx_tbl$`Lib name F`[sample]),x = all_fnFs)]

  sample_idx_tbl$FWD_R2[sample] <-
    all_fnRs[grep(pattern =  paste0("/",sample_idx_tbl$`Lib name F`[sample]),x = all_fnRs)]

  sample_idx_tbl$REV_R1[sample] <-
    all_fnFs[grep(pattern =  paste0("/",sample_idx_tbl$`Lib name R`[sample]),x = all_fnFs)]

  sample_idx_tbl$REV_R2[sample] <-
    all_fnRs[grep(pattern =  paste0("/",sample_idx_tbl$`Lib name R`[sample]),x = all_fnRs)]

}


# Adicionando na tabela sample_idx_table as colunas FWD_R1_paired, FWD_R2_paired, REV_R1_paired e REV_R2_paired:
sample_idx_tbl <- sample_idx_tbl %>%
  mutate("FWD_R1_paired" = "fwd_R1_paired",
         "FWD_R2_paired" = "fwd_R2_paired",
         "REV_R1_paired" = "rev_R1_paired",
         "REV_R2_paired" = "rev_R2_paired")


# Criando o diretório /data/paired para armazenar as reads pareadas que forem recuperadas:
dir.create(path = paste0(pipe_libs,"/paired"),showWarnings = TRUE) 


# Preenchendo na tabela sample_idx_table as colunas relativas às reads pareadas FWD_R1_paired, FWD_R2_paired, REV_R1_paired e REV_R2_paired com os nomes das reads que serão pareadas:
for (sample in 1:nrow(sample_idx_tbl)) {

  sample_idx_tbl$FWD_R1_paired[sample] <-
    all_fnFs[grep(pattern =  paste0("/",sample_idx_tbl$`Lib name F`[sample]),x = all_fnFs)] %>% str_replace(pattern = "raw",replacement = "analises/li_22mar22/data/reads/paired")

  sample_idx_tbl$FWD_R2_paired[sample] <-
    all_fnRs[grep(pattern =  paste0("/",sample_idx_tbl$`Lib name F`[sample]),x = all_fnRs)] %>% str_replace(pattern = "raw",replacement = "analises/li_22mar22/data/reads/paired")

  sample_idx_tbl$REV_R1_paired[sample] <-
    all_fnFs[grep(pattern =  paste0("/",sample_idx_tbl$`Lib name R`[sample]),x = all_fnFs)] %>% str_replace(pattern = "raw",replacement = "analises/li_22mar22/data/reads/paired")

  sample_idx_tbl$REV_R2_paired[sample] <-
    all_fnRs[grep(pattern =  paste0("/",sample_idx_tbl$`Lib name R`[sample]),x = all_fnRs)] %>% str_replace(pattern = "raw",replacement = "analises/li_22mar22/data/reads/paired")

}


# Nomeando as reads com o nome das amostras ----
{
  names(sample_idx_tbl$FWD_R1) <- sample_idx_tbl$File_name
  names(sample_idx_tbl$FWD_R2) <- sample_idx_tbl$File_name
  names(sample_idx_tbl$REV_R1) <- sample_idx_tbl$File_name
  names(sample_idx_tbl$REV_R2) <- sample_idx_tbl$File_name
  names(sample_idx_tbl$FWD_R1_paired) <- sample_idx_tbl$File_name
  names(sample_idx_tbl$FWD_R2_paired) <- sample_idx_tbl$File_name
  names(sample_idx_tbl$REV_R1_paired) <- sample_idx_tbl$File_name
  names(sample_idx_tbl$REV_R2_paired) <- sample_idx_tbl$File_name
}

# Concatenações ----
{
  # concatenando os endereços das reads R1 no objeto reads_fnFs
  reads_fnFs <- c(sample_idx_tbl$FWD_R1,sample_idx_tbl$REV_R1)

  # concatenando os endereços das reads R2 no objeto reads_fnRs
  reads_fnRs <- c(sample_idx_tbl$FWD_R2,sample_idx_tbl$REV_R2)

  # concatenando os endereços das reads R1 pareadas no objeto reads_fnFs_paired
  reads_fnFs_paired <- c(sample_idx_tbl$FWD_R1_paired,sample_idx_tbl$REV_R1_paired)

  # concatenando os endereços das reads R1 pareadas no objeto reads_fnRs_paired
  reads_fnRs_paired <- c(sample_idx_tbl$FWD_R2_paired,sample_idx_tbl$REV_R2_paired)
}


# Conferindo os tamanhos dos vetores. Devem ter o mesmo tamanho:
{
  length(reads_fnFs)
  length(reads_fnRs)
  length(reads_fnFs_paired)
  length(reads_fnRs_paired)
}


# Conferindo os nomes dos vetores. Devem ter os mesmos nomes, neste caso, o nome das amostras
{
  names(reads_fnFs)
  names(reads_fnRs)
  names(reads_fnFs_paired)
  names(reads_fnRs_paired)

}

# Verificar se cada arquivo com o mesmo índice possui o mesmo nome relacionado à amostra correspondente (1:24)
{
  reads_fnFs[20]
  reads_fnRs[20]
  reads_fnFs_paired[20]
  reads_fnRs_paired[20]
}

```

### Filtragem e o repareamento das reads com DADA2

Etapa prévia de filtragem e repareamento das reads com o DADA2.

O comando fastqPairedFilter do pacote DADA2 será utilizado a partir dos
seguintes parâmetros:

-   maxN = c(0,0): descarte das reads com N's

-   maxEE = c(2,2): remoção das reads com taxa de erro maior que 2

-   matchIDs = True: remoção das reads não-pareadas

-   isPhiX = True: descarte das reads de PhiX

Outras flags:

-   'compress = True' irá determinar que os arquivos de saída serão
    .gzip

-   'verbose = TRUE' determina que mensagens de erro irão aparecer

```{r, eval=FALSE,echo=TRUE}

# Filtrando as reads demultiplexadas ----

for (lib in 1:length(reads_fnFs)) {

  print(paste0("Working on file ",lib," of ",length(reads_fnFs),": ",names(reads_fnFs[lib])))

    dada2::fastqPairedFilter(fn = c(reads_fnFs[lib],reads_fnRs[lib]),
                           fout = c(reads_fnFs_paired[lib],reads_fnRs_paired[lib]),
                           maxN = c(0,0),
                           maxEE = c(2,2),
                           matchIDs = TRUE,
                           rm.phix = TRUE,
                           compress = TRUE,
                           verbose = TRUE)
}


  # Preste atenção aos avisos, eles irão indicar quais tags estão com reads vazias ou ausentes

```

### Definindo o nível das amostras

Definindo a ordem que o nome das amostras irão aparecer nos gráficos.

```{r, eval=FALSE,echo=TRUE}

# Definindo os níveis das amostras ----

sample_idx_tbl$File_name %>% unique() %>% sort() %>% paste0(collapse = '",\n"') %>% cat()

sample_levels <- c(
"L1_nov_dec_20_mi",
"L1_nov_dec_20_neo",
"L2_nov20",
"L2_dez20",
"L1_out21",
"L2_out21",
"L3_out21",
"L4_out21",
"L1_nov21",
"L2_nov21",
"L3_nov21",
"L4_nov21"
)
```

## Trabalhando com as reads limpas

Agora que as reads estão limpas e repareadas, podemos iniciar o workflow
do DADA2. A primeira etapa é aprender as taxas de erro específicas por
read, que irão orientar a dereplicação. A dereplicação é o processo de
agrupar as ASVs como sequências únicas.

```{r, echo=TRUE, eval=FALSE}

## Checar a qualidade das reads pareadas
dada2::plotQualityProfile(reads_fnFs_paired[1:24])

## Se a tolerância aos erros for muito baixa, (ie. -e 0), alguns arquivos deixarão de existir.

file.exists(sample_idx_tbl$FWD_R1_paired)
file.exists(sample_idx_tbl$REV_R1_paired)
file.exists(sample_idx_tbl$FWD_R2_paired)
file.exists(sample_idx_tbl$REV_R2_paired)


## Verificando quais amostras possuem arquivos faltando
sample_idx_tbl$FWD_R1_paired[!file.exists(sample_idx_tbl$FWD_R1_paired)]
sample_idx_tbl$REV_R1_paired[!file.exists(sample_idx_tbl$REV_R1_paired)]
sample_idx_tbl$FWD_R2_paired[!file.exists(sample_idx_tbl$FWD_R2_paired)]
sample_idx_tbl$REV_R2_paired[!file.exists(sample_idx_tbl$REV_R2_paired)]


## Aprendendo as taxas de erro ----
{
  ## R1 
run_errF <- learnErrors(c(sample_idx_tbl$FWD_R1_paired[file.exists(sample_idx_tbl$FWD_R1_paired)],
                           sample_idx_tbl$REV_R1_paired[file.exists(sample_idx_tbl$REV_R1_paired)]),
                         multithread=TRUE,randomize = TRUE)
  ## R2
run_errR <- learnErrors(c(sample_idx_tbl$FWD_R2_paired[file.exists(sample_idx_tbl$FWD_R2_paired)],
                           sample_idx_tbl$REV_R2_paired[file.exists(sample_idx_tbl$REV_R2_paired)]),
                         multithread=TRUE,randomize = TRUE)
}


```

### Dereplicação: agrupando as sequências em ASVs

Nesta etapa cada bibilioteca é reduzida a suas sequências únicas e suas contagens.

```{r, eval=FALSE}

## Dereplicação do DADA ----

## Reads R1
{
  ## Reads dos arquivos R1, de orientação FWD
  LGC_run_derep_FWD_R1 <- derepFastq(sample_idx_tbl$FWD_R1_paired[file.exists(sample_idx_tbl$FWD_R1_paired)], verbose=TRUE)
  
  ## Reads dos arquivos R1, de orientação REV
  LGC_run_derep_REV_R1 <- derepFastq(sample_idx_tbl$REV_R1_paired[file.exists(sample_idx_tbl$REV_R1_paired)], verbose=TRUE)
}

## Reads R2
{
  ## Reads dos arquivos R2, de orientação FWD
  LGC_run_derep_FWD_R2 <- derepFastq(sample_idx_tbl$FWD_R2_paired[file.exists(sample_idx_tbl$FWD_R2_paired)], verbose=TRUE)
  
  ##  Reads dos arquivos R2, de orientação REV
  LGC_run_derep_REV_R2 <- derepFastq(sample_idx_tbl$REV_R2_paired[file.exists(sample_idx_tbl$REV_R2_paired)], verbose=TRUE)
}
  
  
## Visualizar quantas reads foram agrupadas em sequências únicas em cada arquivo

  ## R1 FWD
  LGC_run_FWD_dadaFs <- dada(LGC_run_derep_FWD_R1, err=run_errF, multithread=TRUE)
  ## R1 REV
  LGC_run_REV_dadaFs <- dada(LGC_run_derep_REV_R1, err=run_errF, multithread=TRUE)
  ## R2 FWD
  LGC_run_FWD_dadaRs <- dada(LGC_run_derep_FWD_R2, err=run_errR, multithread=TRUE)
   ## R2 REV 
  LGC_run_REV_dadaRs <- dada(LGC_run_derep_REV_R2, err=run_errR, multithread=TRUE)
}


```

### Unindo as sequências em ASVs

???????

```{r, eval=FALSE}

# 15 - merge read pairs ----
LGC_run_FWD_dadaFs
LGC_run_REV_dadaFs
LGC_run_FWD_dadaRs
LGC_run_REV_dadaRs

#run5 FWD ----
run_mergers_FWD <- mergePairs(dadaF = LGC_run_FWD_dadaFs,
                               derepF = LGC_run_derep_FWD_R1,
                               dadaR = LGC_run_FWD_dadaRs,
                               derepR = LGC_run_derep_FWD_R2,
                               minOverlap = 20,
                               maxMismatch = 0,   # can be changed from 0 to 1 if missing pairs. Usualy not needed on good quality runs.
                               returnReject = FALSE,
                               verbose=TRUE)


#run5 REV ----
run_mergers_REV <- mergePairs(dadaF = LGC_run_REV_dadaFs,
                               derepF = LGC_run_derep_REV_R1,
                               dadaR = LGC_run_REV_dadaRs,
                               derepR = LGC_run_derep_REV_R2,
                               minOverlap = 20,
                               maxMismatch = 0,   # can be changed from 0 to 1 if missing pairs. Usualy not needed on good quality runs.
                               returnRejects = FALSE,
                               verbose=TRUE)


length(run_mergers_FWD)
length(run_mergers_REV)
```


# Seqtab

Now, since we have more than one read file pair with the same sample
name, we will use a customized dada2 function
(<https://github.com/benjjneb/dada2/issues/132>).

```{r, echo=TRUE, eval=FALSE}
sumSequenceTables <- function(table1, table2, ..., orderBy = "abundance") {
  # Combine passed tables into a list
  tables <- list(table1, table2)
  tables <- c(tables, list(...))
  # Validate tables
  if(!(all(sapply(tables, dada2:::is.sequence.table)))) {
    stop("At least two valid sequence tables, and no invalid objects, are expected.")
  }
  sample.names <- rownames(tables[[1]])
  for(i in seq(2, length(tables))) {
    sample.names <- c(sample.names, rownames(tables[[i]]))
  }
  seqs <- unique(c(sapply(tables, colnames), recursive=TRUE))
  sams <- unique(sample.names)
  # Make merged table
  rval <- matrix(0L, nrow=length(sams), ncol=length(seqs))
  rownames(rval) <- sams
  colnames(rval) <- seqs
  for(tab in tables) {
    rval[rownames(tab), colnames(tab)] <- rval[rownames(tab), colnames(tab)] + tab
  }
  # Order columns
  if(!is.null(orderBy)) {
    if(orderBy == "abundance") {
      rval <- rval[,order(colSums(rval), decreasing=TRUE),drop=FALSE]
    } else if(orderBy == "nsamples") {
      rval <- rval[,order(colSums(rval>0), decreasing=TRUE),drop=FALSE]
    }
  }
  rval
}

# Generate sequence tables ----

# FWD & REV oriented mergers
run_seqtab_FWD <- makeSequenceTable(samples = run_mergers_FWD)
run_seqtab_REV <- makeSequenceTable(samples = run_mergers_REV)


colnames(run_seqtab_REV) <- dada2:::rc(colnames(run_seqtab_REV))
# colnames(run_seqtab_REV2) <- dada2:::rc(colnames(run_seqtab_REV2))


#atenção. tem que inverter o R2 pra comar as tabelas aqui, antes do merge!!!!!!!!!!!

mergers_seqtab <- sumSequenceTables(table1 = run_seqtab_FWD, table2 = run_seqtab_REV)
# mergers_seqtab2 <- sumSequenceTables(table1 = run_seqtab_FWD, table2 = run_seqtab_REV2)


dim(run_seqtab_FWD)
dim(run_seqtab_REV)
dim(mergers_seqtab)
# dim(mergers_seqtab2)



# R1/R2 unmerged ----

FWD_dadaFs_seqtab <- makeSequenceTable(samples = LGC_run_FWD_dadaFs)
REV_dadaFs_seqtab <- makeSequenceTable(samples = LGC_run_REV_dadaFs)

# colnames(REV_dadaFs_seqtab) <- dada2:::rc(colnames(REV_dadaFs_seqtab))

FWD_dadaRs_seqtab <- makeSequenceTable(samples = LGC_run_FWD_dadaRs)
REV_dadaRs_seqtab <- makeSequenceTable(samples = LGC_run_REV_dadaRs)


#check object
dada2:::is.sequence.table(run_seqtab_FWD)
dada2:::is.sequence.table(run_seqtab_REV)
dada2:::is.sequence.table(R1_seqtab)
dada2:::is.sequence.table(R2_seqtab)
dada2:::is.sequence.table(mergers_seqtab)

length(run_seqtab_REV)
length(run_seqtab_FWD)
length(R1_seqtab)
length(R2_seqtab)
length(mergers_seqtab)


dim(run_seqtab_REV)
dim(run_seqtab_FWD)
#dim(R1_R2_seqtab)
dim(all_seqtab)
dim(mergers_seqtab)
# View(seqtab)
str(all_seqtab)
str(mergers_seqtab)

sample_idx_tbl$File_name %>% unique()


# Inspect distribution of sequence lengths
table(nchar(getSequences(run_seqtab_REV))) %>% plot()
table(nchar(getSequences(run_seqtab_FWD))) %>% plot()
table(nchar(getSequences(mergers_seqtab))) %>% plot()

```

```{r, eval=FALSE}
#TODO
#generate automatic plots for seqtabs

```


```{r, eval=FALSE}
# 16 - remove chimeras ----

mergers_seqtab.nochim <- removeBimeraDenovo(mergers_seqtab, method="consensus", multithread=TRUE, verbose=TRUE)  #minFoldParentOverAbundance??
dim(mergers_seqtab.nochim)
sum(mergers_seqtab.nochim)/sum(mergers_seqtab) # =  0.9567811 , perda de 4.4% na abundancia -> estes 4.4% são quimeras
#count proportion of ASVs of a given length
table(nchar(getSequences(mergers_seqtab.nochim)))
table(nchar(getSequences(mergers_seqtab.nochim))) %>% plot()
rownames(mergers_seqtab.nochim)
rownames(mergers_seqtab)

```


### Count reads and remaining ASVs

```{r, eval=FALSE}
# 17 - count reads proportion throughout the pipeline ----


colnames(sample_idx_tbl_wide)

sample_idx_tbl <- sample_idx_tbl_wide %>%
  pivot_longer(cols = c(FWD_R1, FWD_R2, REV_R1, REV_R2,
                        FWD_R1_paired, FWD_R2_paired,
                        REV_R1_paired, REV_R2_paired),
               names_to = "Stage", values_to = "Read file")


sample_idx_tbl %>% colnames()

#preparing tables with named rows to combine later

#counting reads in raw files

raw_reads <- sample_idx_tbl %>% filter(Stage %in% c("FWD_R1",
                                                    "FWD_R2",
                                                    "REV_R1",
                                                    "REV_R2"))



# 
getN <- function(x) sum(getUniques(x))


#preparing subtables with named rows to combine latter


#raw files

names(sample_idx_tbl$`Read file`) <- sample_idx_tbl$File_name 

raw_reads <- sample_idx_tbl %>% filter(Stage %in% c("FWD_R1","FWD_R2","REV_R1","REV_R2")) 

raw_reads_counts <- ShortRead::countFastq(dirPath = raw_reads$`Read file`) %>% as_tibble(rownames = "Read file")

raw_reads_counts <- raw_reads_counts %>% 
  left_join(y = (raw_reads %>%  mutate(`Read file` = basename(`Read file`)) 
                                                         ),by = "Read file") %>% 
  select(!c( `Read file`,nucleotides,scores))


# raw reads counts ----
tbl_raw_FWD_R1 <- raw_reads_counts[raw_reads_counts$Stage %in% c("FWD_R1"),] %>%
  select(`File_name`, records) %>%
  `colnames<-`(c("File_name", "Raw FWD_R1"))

tbl_raw_FWD_R2 <- raw_reads_counts[raw_reads_counts$Stage %in% c("FWD_R2"),] %>%
  select(`File_name`, records) %>%
  `colnames<-`(c("File_name", "Raw FWD_R2"))


tbl_raw_REV_R1 <- raw_reads_counts[raw_reads_counts$Stage %in% c("REV_R1"),] %>%
  select(`File_name`, records) %>%
  `colnames<-`(c("File_name", "Raw REV_R1"))

tbl_raw_REV_R2 <- raw_reads_counts[raw_reads_counts$Stage %in% c("REV_R2"),] %>%
  select(`File_name`, records) %>%
  `colnames<-`(c("File_name", "Raw REV_R2"))


# # Generate tables for denoised reads
#FWD
tbl_Denoised_FWD_R1 <- (sapply(LGC_run_FWD_dadaFs, getN) %>% as_tibble(rownames = "File_name")) %>% `colnames<-`(c("File_name", "Denoised FWD_R1"))

tbl_Denoised_FWD_R2 <- (sapply(LGC_run_FWD_dadaRs, getN) %>% as_tibble(rownames = "File_name")) %>% `colnames<-`(c("File_name", "Denoised FWD_R2"))

#REV
tbl_Denoised_REV_R1 <- (sapply(LGC_run_REV_dadaFs, getN) %>% as_tibble(rownames = "File_name")) %>% `colnames<-`(c("File_name", "Denoised REV_R1"))

tbl_Denoised_REV_R2 <- (sapply(LGC_run_REV_dadaRs, getN) %>% as_tibble(rownames = "File_name")) %>% `colnames<-`(c("File_name", "Denoised REV_R2"))


tbl_Merged_FWD <- (sapply(run_mergers_FWD, getN) %>% as_tibble(rownames = "File_name")) %>% `colnames<-`(c("File_name", "Merged FWD"))

tbl_Merged_REV <- (sapply(run_mergers_REV, getN) %>% as_tibble(rownames = "File_name")) %>% `colnames<-`(c("File_name", "Merged REV"))

#TODO create optional step to use R1 and R2 alone
# tbl_R1_R2_merged <- (rowSums(all_seqtab) %>% as_tibble(rownames = "File_name")) %>% `colnames<-`(c("File_name", "Merged FWD + Merged REV"))


tbl_Merged <- (rowSums(mergers_seqtab) %>% as_tibble(rownames = "File_name")) %>% `colnames<-`(c("File_name", "Unique Merged"))


tbl_Non_chimeric <- (rowSums(mergers_seqtab.nochim) %>% as_tibble(rownames = "File_name")) %>% `colnames<-`(c("File_name", "Non-chimeric"))


# combine all counts by sample to plot

all_track <- tbl_raw_FWD_R1 %>%
  left_join(tbl_raw_FWD_R2, by = "File_name") %>%
  left_join(tbl_raw_REV_R1, by = "File_name") %>%
  left_join(tbl_raw_REV_R2, by = "File_name") %>%
  left_join(tbl_Denoised_FWD_R1, by = "File_name") %>%
  left_join(tbl_Denoised_FWD_R2, by = "File_name") %>%
  left_join(tbl_Denoised_REV_R1, by = "File_name") %>%
  left_join(tbl_Denoised_REV_R2, by = "File_name") %>%
  left_join(tbl_Merged_FWD, by = "File_name") %>%
  left_join(tbl_Merged_REV, by = "File_name") %>%
  left_join(tbl_Merged, by = "File_name") %>%
  # left_join(tbl_R1_R2_merged,by = "File_name") %>% 
  left_join(tbl_Non_chimeric, by = "File_name") %>% 
  # left_join(tbl_Non_chi_R1, by = "File_name") %>% 
  # left_join(tbl_Non_chi_R2, by = "File_name") %>% 
  left_join(unique(primers_n_samples[c("Primer","File_name")]),by = "File_name") 


colnames(all_track) %>%  paste0(collapse = '",\n"') %>% cat


all_track <- all_track %>% select(c("File_name", "Primer",
                                    "Raw FWD_R1", "Raw FWD_R2", 
                                    "Raw REV_R1", "Raw REV_R2",
                                    "Denoised FWD_R1", "Denoised FWD_R2",
                                    "Denoised REV_R1", "Denoised REV_R2", 
                                    "Merged FWD", "Merged REV",
                                    "Unique Merged",
                                    # "R1 + R2 + merged non-chimeric",
                                    # "Non-chi_R1",
                                    # "Non-chi_R2",
                                    "Non-chimeric"))





#save counts table

writexl::write_xlsx(x = all_track,
                    path = paste0(results_path,"/",prjct_rad,"-reads_and_seqs_counts-",Sys.Date(),".xlsx"),
                    col_names = TRUE,format_headers = TRUE)


# plot reads proportion troughout the pipeline ----
options(scipen = 5)


#18 - set colors for downstream plots ----

# colors 
scales::show_col(viridis::viridis(n = 5))
scales::show_col(viridis::viridis(n = 5))
# inferno5 <- viridis::inferno(n = 10)[5:9]
inferno8 <- viridis::inferno(n = 8)
turbo10 <- viridis::turbo(n = 10)


colnames(track_tbl)


# transform tibble to long format, better for ggplot
  track_tbl <- track_tbl %>%
  gather(key = "Stage",
        value = "Read counts",
        "Raw FWD_R1", "Raw FWD_R2", 
        "Raw REV_R1", "Raw REV_R2", 
        "Denoised FWD_R1", "Denoised FWD_R2", 
        "Denoised REV_R1", "Denoised REV_R2", 
        "Merged FWD", "Merged REV",
        "Unique Merged",
        # "R1 + R2 + merged non-chimeric", 
        # "Non-chi_R1",
        # "Non-chi_R2",
        "Non-chimeric") %>%
  mutate(Stage = factor(Stage, levels = c(
    # "R1 + R2 + merged non-chimeric",
                                          "Non-chimeric",
                                          # "Non-chi_R1",
                                          # "Non-chi_R2",
                                          "Unique Merged", 
                                          "Merged FWD", "Merged REV", 
                                          "Denoised REV_R2", "Denoised REV_R1",
                                          "Denoised FWD_R2", "Denoised FWD_R1",
                                          "Raw REV_R2","Raw REV_R1", "Raw FWD_R2","Raw FWD_R1"))) 

    options(scipen = 22)
  
    
#plot samples reads/abundances
    
  track_plot <- track_tbl %>% 
    mutate(File_name = factor(File_name, levels = sample_levels)) %>%
      ggplot(aes(y = Stage,x = `Read counts`, 
                 fill = Stage,
                 group = File_name
                 )) +
      geom_bar(stat="identity") +
      geom_hline(yintercept = 300000, col = 1, linetype = 2) +
    # scale_fill_viridis(discrete = TRUE,option = "viridis",alpha = 0.75,) +
    # scale_fill_brewer(palette = "Set2")+
    scale_fill_manual(                     
      values = alpha(colour = inferno8[c(2,3,4,4,5,5,5,5,6,6,6,6)],
                     
                                                          alpha =  0.75)) +
    labs(title = paste0(prjct_rad," - eDNA & Ovos e Larvas - Análise: ",Sys.Date()),
         subtitle = "Número de reads por biblioteca e etapa do processamento",
         x = "Número de reads",
         y = "Etapas de processamento dos dados")+
      # xlab(label = "Número de reads")+
      # ylab(label = "Etapas de processamento dos dados") +
      # ggtitle(label = "Ecomol 1st run",
              # subtitle = "Número de reads por por biblioteca e etapa do processamento") +
      facet_wrap(~File_name,ncol = ) +
    coord_fixed(ratio = 25000) +
      theme_bw(base_size = 8) +
    theme(axis.text.x = element_text(angle = 90,hjust = 0.0001,vjust = -0.00000000001,face = "bold")) +
    theme(legend.position = "bottom") +
    theme(axis.title = ggtext::element_markdown())

track_plot 

# save plot
# ggsave(file = paste0(figs_path,"/",prjct_rad,"track_samples.png"),
#      plot = track_plot,
#      device = "png",
#      width = 45,
#      height = 45,
#      units = "cm",
#      dpi = 600)

ggsave(file = paste0(figs_path,"/",prjct_rad,"track_samples.pdf"),
     plot = track_plot,
     device = "pdf",
     width = 55,
     height = 60,
     units = "cm",
     dpi = 600)




track_tbl$`Full name` %>% unique()

```

<!-- ![gráfico de reads](/home/gabriel/projetos/peixes-eDNA/analyses/LGC_run5_22dez21/results/figs/LGC_run5_22dez21track_samples.ng) -->

### Classify taxonomy

On this step the ASVs identified by the **DADA2** pipeline, jointly for
all libraries of each primer, are associated (or not) to any of the
sequences on the Reference 12S Sequences Database. DADA2 has two
strategies to identify taxa. The first, *assignSpecies*, identify
perfect matches of the ASVs in the Reference Database. The second,
*assignTaxonomy*, use a RDP Naive Bayesian Classifier algorithm (Wang,
2007) with kmer size 8 and 100 bootstrap replicates to associate ASVs to
the Reference Database Sequences. In the latter, the taxonomy ranks
classification is proportional to the sequence similarity, although this
relation is not yet clear to us.

```{r, eval=FALSE}

#19 - classify taxonomy exactly ----

mergers_sps <- dada2::assignSpecies(seqs = mergers_seqtab.nochim, allowMultiple = 10,
                                refFasta =  "/home/heron/prjcts/fish_eDNA/data/refs/db/LGC/dez21/dada_tax_fullDB_order_SPs_dez21.fasta",
                                tryRC=TRUE,
                                n = 20000,
                                verbose = TRUE)

  
)

View(mergers_sps)

```

```{r, eval=FALSE}
## Classify taxonomy ----
mergers_taxa <- dada2::assignTaxonomy(seqs = mergers_seqtab.nochim,
                                  refFasta =  "/home/heron/prjcts/fish_eDNA/data/refs/db/LGC/dez21/dada_tax_fullDB_order_dez21.fasta",
                                  multithread=TRUE, tryRC=TRUE,taxLevels = c("Kingdom","Phylum","Class","Order","Family", "Genus", "Species","Specimen","Basin"),
                           outputBootstraps = TRUE, verbose = TRUE )


mergers_csv_sp <- mergers_sps %>% as_tibble() %>% mutate(OTU = rownames(mergers_sps))

all_csv_sp <- bind_rows(mergers_csv_sp)


colnames(all_csv_sp) <- c("exact Genus", "exact Species", "ASV")


mergers_csv_taxa <- mergers_taxa$tax %>% as_tibble() %>% mutate(ASV = rownames(mergers_taxa))


View(mergers_csv_taxa)

mergers_csv_taxa$Species %>%  unique()

```


Here the **DADA2** pipeline ends.

\#\# Phyloseq

On this step the ASVs associated to taxonomic ranks by **DADA2** and
their respective counts by library, are combined using the **Phyloseq**
package.

\#\#\# Generate sample metadata table

Here the experiment metadata is associated to each sample.

```{r, eval=FALSE}
# 22 - create sample table ----


colnames(primers_n_samples)
colnames(primers_n_samples) %>% paste0(collapse = '",\n"') %>% cat
unique(sample_idx_tbl)


samdf <- primers_n_samples[,c(
  "Run",
"Coleta",
"Sample Name",
"File_name",
"Type",
"Point",
"Filter",
"Num replicates",
"Obs",
"Primer",
# "Tag pairs",
# "Tag FWD",
# "Tag REV",
"Control"
                              )] 
samdf <- samdf %>% as.data.frame()
  rownames(samdf) <- samdf$File_name
```


This sample metadata table must be customized for each experiment.


### **Phyloseq** data interpretation

```{r, eval=FALSE}
#23 - interpret dada on phyloseq ----

#o Phyloseq é utilizado para reunir em um mesmo objeto os datasets otu_table, sample_data e tax_table. Ele gera um objeto (neste caso o mergers_ps) que é de dificil manipulacao, e por isso depois ele sera 'derretido' em um objeto mais manipulavel. Ele foi incluido no pipeline por ja ser usado associado com o dada2 

mergers_ps <- phyloseq::phyloseq(phyloseq::otu_table(mergers_seqtab.nochim, taxa_are_rows = FALSE),
                                 phyloseq::sample_data(samdf),
                                 phyloseq::tax_table(mergers_taxa$tax))

View(mergers_seqtab.nochim)
View(samdf)
View(mergers_taxa$tax)

```


### Merge and Flex Phyloseq results

Many different graphics can be generated, together or in isolation, for
all primers/libraries and taxonomic ranks.

```{r, eval=FALSE}

#TODO terminar de implementar a analise de cada read separada
#melt phyloseq object into tbl
mergers_ps_tbl <- psmelt(mergers_ps) %>% as_tibble() %>% mutate(Read_origin = "merged") %>%  filter(Abundance >=1)


        mergers_ps_tbl$OTU %>% unique() %>% length()
        R1_ps_tbl$OTU %>% unique() %>% length()
        R2_ps_tbl$OTU %>% unique() %>% length()


#combine ps tables from all ASVs inputs
# all_ps_tbl <- bind_rows(R1_ps_tbl, R2_ps_tbl,mergers_ps_tbl)
all_ps_tbl <- bind_rows(mergers_ps_tbl)
# %>% unique()


all_ps_tbl$OTU %>% unique() %>% length()


#remove ASVs with abundance = 0
# all_ps_tbl <- all_ps_tbl %>%
#   filter(`Abundance` > 0)



colnames(all_ps_tbl)[colnames(all_ps_tbl) == "OTU"] <- "ASV"


unique(all_ps_tbl$ASV)
# unique(neo_ps_tbl$ASV)
# unique(mif_ps_tbl$ASV)


all_ps_tbl$File_name %>%  unique()
# all_ps_tbl$Primer %>%  unique()




# all_ps_tbl$Full.name <- factor(all_ps_tbl$Full.name, levels = sample_levels)
all_ps_tbl$File_name <- factor(all_ps_tbl$File_name, levels = sample_levels)
      # all_ps_tbl$Primer <- factor(all_ps_tbl$Primer)
      # all_ps_tbl$Point <- factor(all_ps_tbl$Point, levels = c("IR10", "IR2", "IR3", "Tur", "Cneg_ext", "Cneg_PCR"))
      # all_ps_tbl$Depth <- factor(all_ps_tbl$Depth, levels = c("0", "10", "20", "50", "-"))



#concatenate exact species table


# all_ps_tbl <- left_join(by = "ASV",x=all_ps_tbl,y= mergers_csv_sp)
all_ps_tbl <- left_join(by = "ASV",x=all_ps_tbl,y= all_csv_sp)



```

# blast unidentified ASVs

```{r, eval=FALSE}
# blastn ----
# Annotate all ASVs by blastN


asvs_blast <- all_ps_tbl$ASV %>% unique() %>% as.character()
# %>% as.list()


class(asvs_blast)
# asvs_blast<- all_ps_tbl[c("ASV","ASV header")] %>% unique()

# asvs_blast<- paste0(asvs_blast$`ASV header`,"\n",asvs_blast$ASV)

#
# asvs_blast %>% mutate(
#   "subject" = "subject",
#
# )


# load blast functions ----
{

####### Lucio RQ - Execute shell commands #######
shell_exec <- function(cmd, .envir = parent.frame()) {
  if (!requireNamespace("processx", quietly = TRUE)) {
    rlang::abort(message = "Package `processx` package is not installed.")
  }
  cmd_res <- processx::run(
    command = "bash",
    args = c("-c", glue::glue(cmd, .envir = .envir)), echo_cmd = FALSE
  )
  return(cmd_res)
}
########################

######### function to get fasta names from db based on subjectIDs #############
get_fasta_header <- function(id, db_path = "/data/databases/nt/nt") {
  # id <- "JQ365494.1"
  command <- "blastdbcmd -db {db_path} -entry {id} -outfmt %t"
  result <- shell_exec(cmd = command)
  return(result$stdout)
}
################################################################################

#################### function to run blast for each ASV/ASV ####################
run_blastn <- function(asv, db_path = "/data/databases/nt/nt", num_alignments = 3, num_thread = 40) {
  # blast_cmd <- "echo -e '>seq1\n{asv}' | blastn -db {db_path} -outfmt 6 -perc_identity 95 -qcov_hsp_perc 95 -num_threads {as.character(num_thread)} -num_alignments {as.character(num_alignments)}"
  # blast_cmd <- "echo -e '>seq1\n{asv}' | blastn -db {db_path} -outfmt 6 -max_hsps 1 -perc_identity 95 -qcov_hsp_perc 95 -num_threads {as.character(num_thread)} -num_alignments {as.character(num_alignments)}"
  #TODO implement qcohsp on results
  blast_cmd <- "echo -e '>seq1\n{asv}' | blastn -db {db_path} -outfmt \"6 std qcovhsp\" -max_hsps 1 -perc_identity 95 -qcov_hsp_perc 95 -num_threads {as.character(num_thread)} -num_alignments {as.character(num_alignments)}"
  blast_res <- shell_exec(cmd = blast_cmd)
  return(blast_res)
}

##
get_blastn_results <- function(asv, num_thread = 40) {
  blast_res <- run_blastn(asv, num_thread = num_thread)
  if (blast_res$status != 0) {
    rlang::abort(message = "Blast has not run correctly.")
  }
  `%>%` <- dplyr::`%>%`

  if (blast_res$stdout == "") {
    # rlang::inform(glue::glue("Sequence {asv} not found."))
    df_to_return <- tibble::tibble(`ASV` = asv)
    return(df_to_return)
  }
  blast_table <- blast_res$stdout %>%
    readr::read_delim(delim = "\t",
                      col_names = c("query","subject","indentity","length","mismatches","gaps",
                                    "query start","query end","subject start","subject end",
                                    "e-value","bitscore","qcovhsp"
                                    ),
                      trim_ws = TRUE, comment = "#"
    )

  blast_table$`subject header` <- purrr::map_chr(blast_table$subject, get_fasta_header)
  blast_table <- dplyr::relocate(blast_table, `subject header`)
  blast_table <- tibble::rowid_to_column(blast_table, var = "res")

  blast_table <- tidyr::pivot_wider(blast_table, id_cols = subject,  names_from = res, values_from = seq_len(ncol(blast_table)), names_glue = "{res}_{.value}")

  blast_table <- blast_table %>%
    dplyr::mutate(`ASV` = asv) %>%
    dplyr::relocate(starts_with("3_")) %>%
    dplyr::relocate(starts_with("2_")) %>%
    dplyr::relocate(starts_with("1_")) %>%
    dplyr::relocate(`ASV`)
  return(blast_table)
}
}

# Versões paralelas
cores_to_be_used <- future::availableCores() - 2 # Usar todos os cores -2 = 78

cores_to_be_used

future::plan(future::multisession(workers = cores_to_be_used))


blast_res <- furrr::future_map_dfr(asvs_blast, get_blastn_results, num_thread = 1, .options = furrr::furrr_options(seed = NULL))


#save blast res to file ----
readr::write_csv(blast_res, paste0(results_path,"asv_blastn_res_3hits_",Sys.Date(),".csv"),append = FALSE)


base::save.image(paste0(analysis_path,"/",prjct_rad,"-env_",Sys.Date(),"_blast.RData"))

# blast_res_bckp <- blast_res
# blast_res <- blast_res_bckp

nrow(blast_res)
dim(blast_res)


# blast_res <- blast_res[,c(1:43)]  # ASV:`3_bitscore`      muito estranho, foi pra 155 colunas
#
# colnames(blast_res[,c(44:155)])


blast_res <- blast_res %>%  filter(`1_res` == 1 ) #remover o que não deu nada

#   is.na(blast_res[2:155])

str(blast_res)

class(asvs_blast)

# all_ps_tbl$ASV <-   all_ps_tbl$ASV %>% as.character()

#juntar tabela de ASVS e blastn IDs

all_ps_tbl_blast <- left_join(x = all_ps_tbl,y = blast_res,by = "ASV")


all_ps_tbl_blast <- all_ps_tbl_blast %>% 
  mutate(File_name = factor(File_name,levels = sample_levels),
         # Read_origin = factor(Read_origin,levels = c("merged","R1","R2")),
         Primer = factor(Primer,levels = c("NeoFish","MiFish")),
         ASV = factor(ASV),
         Sample = factor(Sample),
         Run = factor(Run),
         Group = factor(Group),
         Expedition = factor(Expedition),
         Coleta = factor(Coleta),
         Sample.Name = factor(Sample.Name),
         Type = factor(Type),
         Point = factor(Point),
         Sub.point = factor(Sub.point),
         Depth = factor(Depth),
         Num.replicates = factor(Num.replicates),
         Obs = factor(Obs),
         Tag.pairs = factor(Tag.pairs),
         Tag.FWD = factor(Tag.FWD),
         Tag.REV = factor(Tag.REV),
         Control = factor(Control)
         # ,
         # Kingdom = factor(Kingdom),
         # Phylum = factor(Phylum),
         # Class = factor(Class),
         # Order = factor(Order),
         # Family = factor(Family),
         # Genus = factor(Genus),
         # Species = factor(Species),
         # Specimen = factor(Specimen),
         # Basin = factor(Basin),
         # `exact Genus` = factor(`exact Genus`),
         # `exact Species` = factor(`exact Species`)
         )


#patching controls, wont be needed next run



colnames(all_ps_tbl_blast)

#all_ps_tbl_blast_bckp <- all_ps_tbl_blast
#all_ps_tbl_blast <- all_ps_tbl_blast_bckp


```

\#calculate sample abundances ----

```{r, eval=FALSE, echo=TRUE}

all_ps_tbl_blast <- all_ps_tbl_blast %>%
  mutate("Relative abundance to all samples" = 0,
         "Relative abundance on sample" = 0,
         "Sample total abundance" = 0)

abd_total <- sum(all_ps_tbl_blast$Abundance)



all_ps_tbl_blast <- all_ps_tbl_blast %>%
  dplyr::group_by(File_name,Read_origin) %>%        #now the abundance on sample is for merged/R1/R2 separetely
  mutate("Sample total abundance" = sum(Abundance),
         "Relative abundance to all samples" = Abundance/abd_total*100,
         "Relative abundance on sample" = Abundance/`Sample total abundance`*100) %>%
  ungroup()

paste0(colnames(all_ps_tbl_blast),"\n") %>%  cat()


all_ps_tbl_blast %>% 
    # filter(Read_origin %in% c("merged")) %>% 
  select(c("Read_origin","File_name","Relative abundance on sample","Sample total abundance")) %>% View()

```

# FINAL id

```{r,echo=TRUE, eval=FALSE}

#all_ps_tbl_blast_bckp2 <- all_ps_tbl_blast
#all_ps_tbl_blast<- all_ps_tbl_blast_bckp2 


all_ps_tbl_blast <- all_ps_tbl_blast %>%
  mutate(`exact GenSp` = paste(`exact Genus`,`exact Species`,sep=" "))


all_ps_tbl_blast <- all_ps_tbl_blast %>%
  mutate("final ID" = if_else((`exact Species` %in% c(NA,"NA", "NA NA")),
                              if_else((Species %in% c(NA,"NA")),
                                      if_else(Genus %in% c(NA,"NA"),
                                              if_else(Family %in% c(NA,"NA"),
                                                    substr(as.character(`1_subject header`),1,30),
                                                    Family),
                                              Genus),
                                      Species),
                              as.character(`exact GenSp`)))



all_ps_tbl_blast <- all_ps_tbl_blast %>%
mutate(
  Kingdom = factor(Kingdom),
  Phylum = factor(Phylum),
  Class = factor(Class),
  Order = factor(Order),
  Family = factor(Family),
  Genus = factor(Genus),
  Species = factor(Species),
  Specimen = factor(Specimen),
  Basin = factor(Basin),
  `exact Genus` = factor(`exact Genus`),
  `exact Species` = factor(`exact Species`),
  `final ID` = factor(`final ID`)
  )


colnames(all_ps_tbl_blast)[colnames(all_ps_tbl_blast) == "ASV"] <- "ASV (Sequence)"
names(all_ps_tbl_blast)[which(names(all_ps_tbl_blast)== "ASV length")] <- "ASV Size (pb)"


```

### ASVs seqs

```{r,echo=TRUE, eval=FALSE}
#25 - recover all ASVs sequences to prepare fasta ----



#all ----
# giving our seq headers more manageable names (ASV_1, ASV_2...)
all_asv_seqs <- tibble("ASV (Sequence)" = unique(all_ps_tbl_blast$`ASV (Sequence)`))

all_asv_seqs <- all_asv_seqs %>%
  mutate("ASV length" = nchar(`ASV (Sequence)`),
  # mutate("ASV length" = nchar(unfactor(ASV)),
         "ASV header" = as.character(""))

all_asv_seqs <- all_asv_seqs[rev(base::order(all_asv_seqs$`ASV length`)),]

for (i in 1:nrow(all_asv_seqs)) {

  all_asv_seqs$`ASV header`[i] <- paste0(">ASV_", i, "_", all_asv_seqs$`ASV length`[i], "bp")

}


#combine ASV headers and all_ps_tbl
all_ps_tbl_blast <- dplyr::left_join(x = all_ps_tbl_blast,
                                     y = all_asv_seqs,
                                     by = "ASV (Sequence)" )


# making and writing out a fasta of our final ASV seqs with tax
for (asv in 1:nrow(all_asv_seqs)) {

  tax <- all_ps_tbl_blast %>%
      filter(`ASV (Sequence)` == all_asv_seqs$`ASV (Sequence)`[asv]) %>%
    # select("Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species", "Specimen") %>%
    select("Read_origin", "Family", "Genus", "Species", "final ID") %>%
    unique() %>%
    paste0(collapse = "|")

  all_asv_seqs$`ASV header`[asv] <- paste0(all_asv_seqs$`ASV header`[asv],"_",tax)

  # if (condition) {
  # fazer algum teste pra ver ser ta certo
  # }
}

#write fasta file with ASVs and Taxonomy
all_asv_fasta <- c(rbind(all_asv_seqs$`ASV header`, all_asv_seqs$`ASV (Sequence)`))

write(all_asv_fasta, paste0(results_path,"/",prjct_rad,"-all_ASVs_all_primers.fasta"))





all_ps_tbl_blast$Abundance %>% table() %>%  plot()
all_ps_tbl_blast$`Relative abundance on sample` %>% table() %>%  plot()
all_ps_tbl_blast$`ASV Size (pb)` %>% table() %>%  plot()

```

\#\#\#SWARM - ASVs to OTUs

```{r,echo=TRUE, eval=FALSE}

asvs_abd <- all_ps_tbl_blast %>%
  group_by(`ASV (Sequence)`,`ASV header`) %>%
  mutate("ASV total abundance" = sum(Abundance)) %>%
  select(c(`ASV (Sequence)`,`ASV header`,`ASV total abundance`)) %>%
  unique() %>%
  mutate(`ASV header abd` = paste0(`ASV header`,"_",`ASV total abundance`))

#write fasta file with ASVs and Taxonomy
all_asv_fasta_abd <- c(rbind(asvs_abd$`ASV header abd`, asvs_abd$`ASV (Sequence)`))

write(all_asv_fasta_abd, paste0(results_path,"/",prjct_rad,"_abd.fasta"))

#run on bash
# heron@edna:/home/gabriel/projetos/peixes-eDNA/analises/LGC_run5_22dez21/results/swarm$ swarm -t 50 ../LGC_run5_22dez21_abd.fasta -s LGC_run5_22dez21-swarm.stats -o LGC_run5_22dez21-swarm.out -w LGC_run5_22dez21-representative_OTUs.fasta -i LGC_run5_22dez21-swarm.structure -f


swarm_clust <- list.files(path = swarm_path,
                          pattern = "swarm.out",
                          full.names = TRUE ) %>% 
  readr::read_lines()


asvs_abd <- asvs_abd %>% mutate("OTU"= 0)

# Função para mapear as OTUs (linhas do swarm) correspondentes a cada ASV 

find_otu <- function(ASV_header,clusters_swarm){
  
  ASV_OTU_tbl <- tibble::tibble(`ASV header abd` = ASV_header,
                                OTU = 0)
  
  for (line in 1:length(clusters_swarm)) {
    if (stringr::str_detect(string = clusters_swarm[line],
                            pattern = stringr::str_remove_all(string = ASV_header,
                                                              pattern = ">"))) {

      ASV_OTU_tbl$OTU <- line
      }
    }
              return(ASV_OTU_tbl)
} 

# Versões paralelas
cores_to_be_used <- future::availableCores() - 2 # Usar todos os cores -2 = 78
future::plan(future::multisession(workers = cores_to_be_used))


# 
# find_otu(ASV_header = asvs_abd$`ASV header abd`[30],
#          clusters_swarm = swarm_clust)



ASVs_and_OTUs <- furrr::future_map_dfr(asvs_abd$`ASV header`,
                                       clusters_swarm = swarm_clust,
                                       .f = find_otu,
                                       .options = furrr::furrr_options(seed = NULL))


ASVs_and_OTUs$`ASV header abd` %>% unique() %>% length()

ASVs_and_OTUs$OTU %>% unique() %>% length()





asvs_abd <- left_join(asvs_abd,
                      ASVs_and_OTUs,
                      by = "ASV header abd")



all_ps_tbl_blast <- left_join(all_ps_tbl_blast,asvs_abd[,c(1,5)])
# all_ps_tbl_blast <- left_join(select(all_ps_tbl_blast,-c("OTU")),asvs_abd[,c(1,5)])

all_ps_tbl_blast %>% select(`final ID`,OTU) %>% View() 
all_ps_tbl_blast %>% select(`final ID`,OTU,`ASV length`) %>% unique() %>% View() 



all_ps_tbl_blast %>% select(`final ID`,OTU) %>% select(OTU) %>% unique() 
all_ps_tbl_blast %>% select(`ASV (Sequence)`,`final ID`,OTU) %>% unique() 
```

\#Reorder table

```{r, eval=FALSE}

# all_ps_tbl_blast_bckp3 <- all_ps_tbl_blast

# 
# colnames(all_ps_tbl_blast) %>% paste0(collapse = '",\n"') %>% cat()

all_ps_tbl_blast <-
  all_ps_tbl_blast %>%
  select(c(
"Sample",
"Abundance",
"Run",
"Group",
"Expedition",
"Coleta",
"Sample.Name",
"File_name",
"OTU",
"final ID",
"Relative abundance to all samples",
"Relative abundance on sample",
"Sample total abundance",
"Type",
"Point",
"Sub.point",
"Depth",
"Num.replicates",
"Obs",
"Primer",
"Quantidade.de.ovos.ou.larvas",
"Kingdom",
"Phylum",
"Class",
"Order",
"Family",
"Genus",
"Species",
"Specimen",
"Basin",
"Read_origin",
"exact Genus",
"exact Species",
"exact GenSp",
# "1_res",
"1_subject header",
# "1_query",
"1_subject",
"1_indentity",
"1_length",
"1_mismatches",
"1_gaps",
"1_query start",
"1_query end",
"1_subject start",
"1_subject end",
"1_e-value",
"1_bitscore",
"1_qcovhsp",
# "2_res",
"2_subject header",
# "2_query",
"2_subject",
"2_indentity",
"2_length",
"2_mismatches",
"2_gaps",
"2_query start",
"2_query end",
"2_subject start",
"2_subject end",
"2_e-value",
"2_bitscore",
"2_qcovhsp",
# "3_res",
"3_subject header",
# "3_query",
"3_subject",
"3_indentity",
"3_length",
"3_mismatches",
"3_gaps",
"3_query start",
"3_query end",
"3_subject start",
"3_subject end",
"3_e-value",
"3_bitscore",
"3_qcovhsp",
"Tag.pairs",
"Tag.FWD",
"Tag.REV",
"Control",
"ASV length",
"ASV header",
# "ASV header abd",
"ASV (Sequence)"
  ))

# paste0(colnames(all_ps_tbl_blast),"\n") %>%  cat()
# names(all_ps_tbl_blast)[which(names(all_ps_tbl_blast)=="ASV")] <- "ASV (Sequence)"
names(all_ps_tbl_blast)[which(names(all_ps_tbl_blast)== "ASV length")] <- "Size (pb)"


```

\#Identify ASVs present on the Blanks/Negative controls

```{r,echo=TRUE, eval=FALSE}




all_ps_tbl_blast <- all_ps_tbl_blast %>% mutate("Remove" = "Samples only")

sample_levels

all_ps_tbl_blast$Type %>% unique()


# all_ps_tbl_blast$Remove[(all_ps_tbl_blast$Type %in% c("POSITIVE CONTROL","FILTRATION BLANK","EXTRACTION BLANK","PCR BLANK"))] <- "Remove"

all_ps_tbl_blast %>% 
  group_by(Group) %>% 
  mutate()




# split data into two libraries ----
eDNA_ps_tbl_blast <-all_ps_tbl_blast[all_ps_tbl_blast$Group %in% c("eDNA"),]
ichth_ps_tbl_blast <-all_ps_tbl_blast[all_ps_tbl_blast$Group %in% c("Ichthyoplancton"),]


# Identify contamination based on respective controls----


eDNA_ps_tbl_blast$Remove[(eDNA_ps_tbl_blast$Type %in% c("Pos. Control", "Neg. control"))] <- "Controls"

ichth_ps_tbl_blast$Remove[(ichth_ps_tbl_blast$Type %in% c("Pos. Control", "Neg. control"))] <- "Controls"

# ----


#mark control replicates


# 
# eDNA_contam_ASVs <- eDNA_ps_tbl_blast$`ASV (Sequence)`[(eDNA_ps_tbl_blast$Remove %in% c("Controls"))] %>% unique() %>% as.tibble() %>% `colnames<-`("ASV (Sequence)")
# 
# Ichthyo_contam_ASVs <- ichth_ps_tbl_blast$`ASV (Sequence)`[(ichth_ps_tbl_blast$Remove %in% c("Controls"))] %>% unique() %>% as.tibble() %>% `colnames<-`("ASV (Sequence)")



eDNA_contam_ASVs <- eDNA_ps_tbl_blast %>%
  filter(Remove %in% "Controls") %>%
  group_by(`ASV (Sequence)`) %>%
  mutate("Max. ASV abd. in control" = max(`Relative abundance on sample`)) %>%
  ungroup() %>%
  select("ASV (Sequence)","Max. ASV abd. in control") %>%
  unique()



ichth_contam_ASVs <- ichth_ps_tbl_blast %>%
  filter(Remove %in% "Controls") %>%
  group_by(`ASV (Sequence)`) %>%
  mutate("Max. ASV abd. in control" = max(`Relative abundance on sample`)) %>%
  ungroup() %>%
  select("ASV (Sequence)","Max. ASV abd. in control") %>% unique()


all_ps_tbl_blast$`ASV (Sequence)`%>%  unique()
all_ps_tbl_blast$`ASV (Sequence)`[all_ps_tbl_blast$Remove != "Remove"] %>%  unique()
all_ps_tbl_blast$Remove != "Remove" %>%  unique()


for (line in 1:nrow(eDNA_ps_tbl_blast)) {
  for (asv in 1:nrow(eDNA_contam_ASVs)) {
    if (eDNA_ps_tbl_blast$`ASV (Sequence)`[line] == eDNA_contam_ASVs$`ASV (Sequence)`[asv] ) {
      if ((eDNA_ps_tbl_blast$`Relative abundance on sample`[line]) >= (1 *eDNA_contam_ASVs$`Max. ASV abd. in control`[asv]) ) {
        eDNA_ps_tbl_blast$`Abd. higher than in control` <- "Higher than 1x in control"
        eDNA_ps_tbl_blast$Remove <- "Probable contamintion"
      }else{
        eDNA_ps_tbl_blast$`Abd. higher than in control` <- "Lower than 1x in control"
        eDNA_ps_tbl_blast$Remove <- "Probable true detection"
      }
    }

  }

}


for (line in 1:nrow(ichth_ps_tbl_blast)) {
  for (asv in 1:nrow(ichth_contam_ASVs)) {
    if (ichth_ps_tbl_blast$`ASV (Sequence)`[line] == ichth_contam_ASVs$`ASV (Sequence)`[asv] ) {
      if ((ichth_ps_tbl_blast$`Relative abundance on sample`[line]) >= (1 *ichth_contam_ASVs$`Max. ASV abd. in control`[asv]) ) {
        ichth_ps_tbl_blast$`Abd. higher than in control` <- "Higher than 1x in control"
        ichth_ps_tbl_blast$Remove <- "Probable contamintion"
      }else{
        ichth_ps_tbl_blast$`Abd. higher than in control` <- "Lower than 1x in control"
        ichth_ps_tbl_blast$Remove <- "Probable true detection"
      }
    }

  }

}



eDNA_ps_tbl_blast %>% mutate()

ichth_ps_tbl_blast %>% mutate()


#backup
# all_ps_tbl_blast_old <- all_ps_tbl_blast

all_ps_tbl_blast <- bind_rows(eDNA_ps_tbl_blast, ichth_ps_tbl_blast)


all_ps_tbl_blast$`ASV (Sequence)` %>%  unique()
all_ps_tbl_blast$Sample %>%  table()
all_ps_tbl_blast$Remove %>%  table()





```

\#\#\#Salvando as tabelas completas

```{r,echo=TRUE, eval=FALSE}


#order by abundance

smp_abd_ID <- all_ps_tbl_blast[rev(base::order(all_ps_tbl_blast$Abundance)),] %>%
  filter(`Abundance` > 0)

dim(smp_abd_ID)

writexl::write_xlsx(x = smp_abd_ID,
                    path = paste0(results_path,"/",prjct_rad,"-todas_info_da_analise_",Sys.Date(),".xlsx"),
                    col_names = TRUE,format_headers = TRUE)

#CAREFULL WITH THAT RAM EUGENE
(smp_abd_ID$`final ID`[1:200] %>% str_split(pattern = " ", n = 3,simplify = TRUE))[,c(1,2)]
smp_abd_ID$`final ID`[1:200] %>% str_extract(pattern = "^.[[:alnum:]]++.\\S.[[:alnum:]]++")



smp_abd_ID <- smp_abd_ID %>% mutate(
  Identification = str_extract(string = `final ID`,
                               pattern = "^.[[:alnum:]]++.\\S.[[:alnum:]]++"))



smp_abd_ID_summary <- smp_abd_ID %>%  
  filter(`Probable bacteria` == FALSE) %>%
  # filter(Abundance >= 20) %>% 
# group_by(Sample,`final ID`) %>% 
group_by(Type,Sample,`final ID`) %>% 
  summarize(
    Type = unique(Type),
    Primer = unique(Primer),
    Read_origin = unique(Read_origin),
    `Num ASVs` = length(unique(`ASV (Sequence)`)),
    `Num OTUs` = length(unique(`OTU`)),
            # `Species` = unique(Identification),
            `Total Abundance` = sum(Abundance)
            # ,
            # `Different ASVs` = (unique(Replicate))
            ) %>% 
  ungroup()
# %>% 
#   group_by(Sample.name) %>% 
#   mutate(`Replicates` = max())


            
            #summary with R1/R2 and merged
            smp_abd_ID_summary <- smp_abd_ID %>%  
              # filter(Remove == "Keep") %>% 
              # filter(Abundance >= 20) %>% 
            # group_by(Sample,`final ID`) %>% 
            group_by(Sample,Identification,Read_origin) %>% 
              summarize(Primer = unique(Primer),
                        `Num ASVs` = length(unique(`ASV (Sequence)`)),
                        # `Species` = unique(Identification),
                        `Total Abundance` = sum(Abundance),
                        `Minimum Identity` = min(`1_indentity`)
                        # ,
                        # `Different ASVs` = (unique(Replicate))
            )
# %>% 
#   group_by(Sample.name) %>% 
#   mutate(`Replicates` = max())











smp_abd_ID_summary <-  smp_abd_ID %>%  
  filter(Read_origin %in% c("R1")) %>% 
  # group_by(Sample) %>% 
  #  ungroup() %>% 
   # filter(Remove != "Keep") %>%
   # filter(Remove != "Keep") %>%
   # filter(Abundance >= 20) %>% 
   group_by(Sample,`final ID`) %>% 
   summarize(`Num ASVs` = length(unique(`ASV (Sequence)`)),
            `Species` = unique(`final ID`),
            `Total Abundance` = sum(Abundance)
            ) %>% 
  unique()


smp_abd_ID %>% 
  group_by(Primer,Read_origin) %>% 
  summarize(Primer = unique(Primer),
            Read_origin = unique(Read_origin),
            `Num amostras` = length(unique(File_name)),
            `Num ASVs encontradas` = length(unique(`ASV (Sequence)`)),
            `Num ASVs identificadas pelo BLASTn` = length(unique(`ASV (Sequence)`[!is.na(`final ID`)]))) %>% View()










writexl::write_xlsx(x = smp_abd_ID_summary,
                    path = paste0(results_path,"/",prjct_rad,"-summary_",Sys.Date(),".xlsx"),
                    col_names = TRUE,format_headers = TRUE)


smp_abd_ID %>% 
  # filter(Type %in% c("KLABIN")) %>% 
  group_by(Group,Type,Primer,Read_origin) %>% 
  summarize(Type = unique(Type),
            Primer = unique(Primer),
            Read_origin = unique(Read_origin),
            `Num amostras` = length(unique(File_name)),
            `Num ASVs encontradas` = length(unique(`ASV (Sequence)`)),
            `Num OTUs encontradas` = length(unique(`OTU`)),
            `Num ASVs identificadas pelo BLASTn` = length(unique(`ASV (Sequence)`[!is.na(`final ID`)])),
            `Proporção de ASVs identificadas` = (`Num ASVs identificadas pelo BLASTn`/`Num ASVs encontradas`*100),
            `Num ASVs prováveis bactérias` = length(unique(`ASV (Sequence)`[`Probable bacteria` == TRUE])),
            `Proporção de ASVs prováveis bactérias` = (`Num ASVs prováveis bactérias`/`Num ASVs encontradas`*100)
            ) %>%
  writexl::write_xlsx(
    path = paste0(results_path,"/",prjct_rad,"-primer_summary-",Sys.Date(),".xlsx"),
    col_names = TRUE,format_headers = TRUE)


```

```{r echo=FALSE,eval=FALSE}
# all_ps_tbl_blast_bckp5 <- all_ps_tbl_blast


#remove bacterias

all_ps_tbl_blast <- all_ps_tbl_blast %>% mutate("Probable bacteria" = if_else((stringr::str_detect(string = all_ps_tbl_blast$`final ID`,
                            pattern = "ncultured|bacter|proka|16S rRNA amplicon fragment from a soil sample")), TRUE, FALSE))


```
