---
title: "eDNA-LGC Metagenomics Pipeline"
author: "Hilário, OH; Mendes, GA"
date: "abril/2022"
output:
  html_document:  
    code_download: yes
    theme: flatly
    toc: true
    toc_depth: 4
    toc_float: true
  pdf_document: default
editor_options:
  chunk_output_type: console
  markdown: 
    wrap: 72
---

------------------------------------------------------------------------

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load myData, include=FALSE}
load(file = "/home/gabriel/projetos/peixes-eDNA/analises/pipeline-modelo/environment_pipeline-modelo.RData")
```

## Introdução

Este *pipeline* foi desenvolvido pelo Dr. Heron Hilário e comentado e
adaptado pelo aluno de mestrado Gabriel Antônio Mendes. Seu
desenvolvimento ocorreu sob orientação do Professor Dr. Daniel de
Cardoso Carvalho no Laboratório de Genética da Conservação da Pontífice
Universidade Católica de Minas Gerais, Brasil (LGC PUC-MG). Este código
possui como objetivo principal realizar a identificação taxonômica de
amplicons gerados a partir do material genético extraído de amostras
ambientais, como água ou solo. Ele foi desenvolvido tendo em seu núcleo
o uso do pacote dada2, que utiliza do algorítmo **DADA2** para inferir
as sequências exatas das variantes de amplicons (ASVs), a partir de
dados HTS *(High Throughput Sequencig)* gerados por sequenciamentos
Illumina.

Para a realização da inferência taxonômica das ASVs utilizamos de
referência um banco de dados próprio, o db12SLGC, onde estão depositadas
as sequências do gene rRNA mitocondrial 12S das espécies de peixes das
bacias dos Rio Jequitinhonha e São Francisco. O **DADA2** pode não ser
suficiente para realizar a atribuição taxonômica de todos os amplicons
gerados, e por isso também foi implementado no *pipeline* a
identificação a partir do BLAST, realizada simultaneamente. Atualmente,
sabe-se que uma abordagem que utilize apenas a inferência da sequência
exata das ASVs não é suficiente para realizar a identificação adequada.
Por isso, o *eDNA-LGC Metagenomics Pipeline* utiliza também o SWARM para
a realização da inferência de OTUs.

Se você tem alguma dúvida ou sugestão, por favor sinta-se à vontade para
escrever um e-mail para
[lgc.edna\@gmail.com](mailto:lgc.edna@gmail.com){.email}

## Preparação

O primeiro passo é carregar as bibliotecas dos pacotes do *R* que serão
utilizados, os programas do sistema e definir os endereços das pastas
que serão utilizadas.

```{r, eval=FALSE,echo=TRUE}

## Carregando o arquivo de environment do projeto e Rmd e os pacotes necessários

# Carregando pacotes ----
{
  library(dplyr)
  library(tidyr)
  library(tibble)
  library(stringr)
  library(ggplot2)
  library(phyloseq)
  library(Biostrings)
  library(ShortRead)
  library(dada2)
  library(DECIPHER)
  library(future)
}

## Definindo o caminho para o executável do cutadapt
cutadapt <- "/usr/local/bin/cutadapt"


## Definindo o caminho das pastas essenciais para o projeto:

## Definindo o caminhos de saída e de entrada ----

## Usar o mesmo caminho que foi criado na variavel do bash $PRJCT_DIR
analysis_path <- "/home/gabriel/projetos/peixes-eDNA/analises/pipeline-modelo"

# criacao do data_folder
data_path <- paste0(analysis_path,"/data")
if(!dir.exists(data_path)) dir.create(data_path)

## Criacao do diretorio das reads processadas
pipe_libs <- paste0(data_path,"/reads")
if(!dir.exists(pipe_libs)) dir.create(pipe_libs)

## Criação da pasta de resultados
results_path <- paste0(analysis_path,"/results")
if(!dir.exists(results_path)) dir.create(results_path)

## Criação da pasta de figuras
figs_path <- paste0(results_path,"/figs")
if(!dir.exists(figs_path)) dir.create(figs_path)

## Criação da pasta do swarm
swarm_path <- paste0(results_path,"/swarm")
if(!dir.exists(swarm_path)) dir.create(swarm_path)

## Criação do radical do nome do projeto
prjct_rad <-c("pipeline-modelo")

## Garantir que as pastas estão vazias
list.files(analysis_path)

## Indicar onde as reads brutas estão ----
run_path <- "/home/gabriel/projetos/peixes-eDNA/raw"


## Todas as bibliotecas ja estão demultiplexadas
raw_libs <- "/home/gabriel/projetos/peixes-eDNA/raw" # PATH to the directory containing 
raw fastq files (same as in $RAW_DATA).

## Função para conferir se o diretório salvo em raw_libs realmente contém os arquivos que voce quer analisar
list.files(path = raw_libs)
```

## Obtenção dos dados

Download das amostras demultiplexadas do *Illumina BaseSpace Sequence
Hub* utilizando a interface *bs*.

```{bash, eval=FALSE}

## Navegando ate a pasta com os arquivos brutos
cd $raw_data_folder/$run_folder;

## Realizando a autenticacao no BaseSpace Sequence Hub (apenas no primeiro log in)
bs auth;

## Exibindo os sequenciamentos das corridas no seu repositório BaseSpace
bs list datasets;


bs download project -n "fish_metabarcoding_dez21" -o fastq --extension=fastq.gz;

## Organizar os arquivos brutos ----

## Armazenar o caminho dos arquivos brutos na variável
RAW_DATA=/home/heron/runs/run5_22dez21/raw

## Criar a pasta para receber os arquivos brutos
mkdir $RAW_DATA;

## Copiar/mover todos os fastqs para uma pasta raw
mv /home/heron/runs/run5_22dez21/fastq/*/*fastq.gz $RAW_DATA; 
```

## Demultiplexando as amostras

-   Cada biblioteca possui amostras que estão combinadas sob um mesmo
    index Illumina. Agora iremos utilizar as sequências dos primers
    indexados para selecionais quais *reads* correspondem a cada amostra
    única. Como o kit de preparação de bibliotecas utilizado
    (Collibri-ThermoFisher) resulta em *amplicons* sequenciados em ambas
    orientações, iremos partir do princípio que é possível encontrar os
    primers FWD e REV em qualquer conjunto de reads. Para que as
    próximas etapas ocorram adequadamente, deveremos fornecer um arquivo
    .csv correspondente aos metadados associados a todas amostras.

-   Esta tabela deve obrigatoriamente conter três colunas, *File_name*,
    *Tag FWD*, *Tag REV*. Cada *amostra* deve corresponder a um valor
    único em *File_name* e a uma combinação única de *Tag FWD* e *Tag
    REV*.

### Regenerando index+primer

Este trecho possui a entrada mais importante da análise. Para dar
continuidade, é necessário ter a tabela *primers_n\_samples*, que possui
a relação da combinação de primers FWD e REV para cada amostra
sequenciada.

```{r, eval=FALSE,echo=TRUE}

## carregar os indexes dos primers e a tabela de amostras 

## Esta é a entrada mais importante da análise 
## Este input e o arquivo .sv com colunas com valores unicos: File_name, (combined) "Tag FWD-Tag REV"

primers_n_samples <- readr::read_csv(file = "/home/gabriel/projetos/peixes-eDNA/analises/pipeline-modelo/data/LI_primers_n_samples.csv")


## Conferir se há algum nome de arquivo duplicado ----
primers_n_samples$File_name %>% length()
primers_n_samples$File_name %>% unique() %>% length()
primers_n_samples$File_name %>% duplicated() %>% which()
primers_n_samples$File_name[primers_n_samples$File_name %>% duplicated()]

## Conferir se há alguma combinação de tags repetida ----
select(primers_n_samples,c("Tag FWD","Tag REV")) %>% unite(col = "col",sep = "-") %>% nrow()
select(primers_n_samples,c("Tag FWD","Tag REV")) %>% unite(col = "col",sep = "-") %>% unique() %>% nrow()
select(primers_n_samples,c("Tag FWD","Tag REV")) %>% unite(col = "col",sep = "-") %>% duplicated() %>% which()
select(primers_n_samples,c("Tag FWD","Tag REV")) %>% unite(col = "col",sep = "-") %>% duplicated() %>% which()
primers_n_samples$File_name[select(primers_n_samples,c("Tag FWD","Tag REV")) %>% unite(col = "col",sep = "-") %>% duplicated() %>% which()]

## carregar sequências de primers sem Ns ----
primers_seqs <-  Biostrings::readDNAStringSet(filepath = "/home/gabriel/projetos/peixes-eDNA/analises/pipeline-modelo/data/neo_miU_indexed_primers.fasta",format = "fasta")

primers_seqs <- primers_seqs %>% 
  as.data.frame() %>% 
  `colnames<-`("Primer seq") %>% 
  mutate(`Primer name` = rownames(.)) %>% 
  as_tibble()
```

### Cutadapt

Iremos utilizar o pacote **Cutadapt** para realizar o demultiplex. Esse
processo irá separar as *reads* de acordo com cada combinação de
*primers* FWD e REV a partir da tabela fornecida no passo anterior. No
final desta etapa, a pasta *raw* terá um par de arquivos R1 e R2 para
cada combinação de *primers*.

**Referência**:
<https://cutadapt.readthedocs.io/en/stable/guide.html#combinatorial-demultiplexing>

```{bash, eval=FALSE}

## Na edna com
ulimit -n 1000000

## Combine file to demultiplex at once
 cat $RAW_DATA/*22dez21*R1* > $RAW_DATA/LGC_run5_R1.fastq.gz
 cat $RAW_DATA/*22dez21*R2* > $RAW_DATA/LGC_run5_R2.fastq.gz


cutadapt -e 0.10 -j 79 --no-indels --max-n 0\
 -g file:/home/gabriel/projetos/peixes-eDNA/analyses/LGC_run5_22dez21/data/neo_miU_indexed_primers.fasta  \
 -G file:/home/gabriel/projetos/peixes-eDNA/analyses/LGC_run5_22dez21/data/neo_miU_indexed_primers.fasta \
 -o /home/heron/runs/run5_22dez21/dmux_CDI/{name1}-{name2}.R1.fastq \
 -p /home/heron/runs/run5_22dez21/dmux_CDI/{name1}-{name2}.R2.fastq \
 /home/heron/runs/run5_22dez21/raw/LGC_run5_R1.fastq.gz \
 /home/heron/runs/run5_22dez21/raw/LGC_run5_R2.fastq.gz \
 2> /home/heron/runs/run5_22dez21/eDNA_LGC_run5_cut_010_noNs.txt;
 
 ls -lahSr /home/heron/runs/run5_22dez21/dmux_CDI/| grep -v ;

## Remover arquivos vazios (don't, it will generate conflicts soon)
find ~/runs/run_28out21/dmux_CDI$ \
 -size 0 \
 -delete;
```

## Preparação dos arquivos brutos para o DADA2

### Preparando os arquivos

Os arquivos na pasta *raw* devem ser renomeados de acordo com sua
amostra correspondente, informação que também está contida na tabela
*primers_n\_samples*.

```{r, eval=FALSE,echo=TRUE}

## Criação do objeto libs_path com o caminho das bibliotecas demultiplexadas na eDNA: 
libs_path <- "/home/gabriel/projetos/peixes-eDNA/raw"
  

all_fnFs <- sort(list.files(libs_path, pattern=".R1.fastq", full.names = TRUE))
all_fnRs <- sort(list.files(libs_path, pattern=".R2.fastq", full.names = TRUE))

## Confira se ambos os objetos possuem tamanhos idênticos:
length(all_fnFs) 
length(all_fnRs)

## Carregando os dados das amostras (origem e indexes)

colnames(primers_n_samples)

sample_idx_tbl <- primers_n_samples %>% select(c("File_name", "Tag FWD", "Tag REV"))

## Verificar se existem amostras com nomes repetidos:
sample_idx_tbl$File_name %>% unique()
sample_idx_tbl$File_name[sample_idx_tbl$File_name %>% duplicated()]

## Não devem haver amostras com nomes iguais!! 
## Caso existam, editar na tabela primers_n_samples!


## Criação das colunas Lib name F a partir dos indexes FWD e REV de cada amostra, contida em primers_n_samples:
sample_idx_tbl <- sample_idx_tbl %>%
  unite(col = `Lib name F`,c(`Tag FWD`,`Tag REV`), sep = "-", remove = FALSE) %>%
  unite(col = `Lib name R`,c(`Tag REV`,`Tag FWD`), sep = "-", remove = FALSE)

## Adicionando na tabela sample_idx_table as colunas FWD_R1, FWD_R2, REV_R1 e REV_R2:
sample_idx_tbl <- sample_idx_tbl %>%
  mutate("FWD_R1" = "F-R1",
         "FWD_R2" = "F-R2",
         "REV_R1" = "R-R1",
         "REV_R2" = "R-R2")

## Substituindo o conteúdo das colunas FWD_R1, FWD_R2, REV_R1 e REV_R2 pelo endereço de cada amostra na pasta peixes-eDNA/raw
for (sample in 1:nrow(sample_idx_tbl)) {

  sample_idx_tbl$FWD_R1[sample] <-
    all_fnFs[grep(pattern =  paste0("/",sample_idx_tbl$`Lib name F`[sample]),x = all_fnFs)]

  sample_idx_tbl$FWD_R2[sample] <-
    all_fnRs[grep(pattern =  paste0("/",sample_idx_tbl$`Lib name F`[sample]),x = all_fnRs)]

  sample_idx_tbl$REV_R1[sample] <-
    all_fnFs[grep(pattern =  paste0("/",sample_idx_tbl$`Lib name R`[sample]),x = all_fnFs)]

  sample_idx_tbl$REV_R2[sample] <-
    all_fnRs[grep(pattern =  paste0("/",sample_idx_tbl$`Lib name R`[sample]),x = all_fnRs)]

}


# Adicionando na tabela sample_idx_table as colunas FWD_R1_paired, FWD_R2_paired, REV_R1_paired e REV_R2_paired:
sample_idx_tbl <- sample_idx_tbl %>%
  mutate("FWD_R1_paired" = "fwd_R1_paired",
         "FWD_R2_paired" = "fwd_R2_paired",
         "REV_R1_paired" = "rev_R1_paired",
         "REV_R2_paired" = "rev_R2_paired")


# Criando o diretório /data/paired para armazenar as reads pareadas que forem recuperadas:
dir.create(path = paste0(pipe_libs,"/paired"),showWarnings = TRUE) 


# Preenchendo na tabela sample_idx_table as colunas relativas às reads pareadas FWD_R1_paired, FWD_R2_paired, REV_R1_paired e REV_R2_paired com os nomes das reads que serão pareadas:
for (sample in 1:nrow(sample_idx_tbl)) {

  sample_idx_tbl$FWD_R1_paired[sample] <-
    all_fnFs[grep(pattern =  paste0("/",sample_idx_tbl$`Lib name F`[sample]),x = all_fnFs)] %>% str_replace(pattern = "raw",replacement = "analises/pipeline-modelo/data/reads/paired")

  sample_idx_tbl$FWD_R2_paired[sample] <-
    all_fnRs[grep(pattern =  paste0("/",sample_idx_tbl$`Lib name F`[sample]),x = all_fnRs)] %>% str_replace(pattern = "raw",replacement = "analises/pipeline-modelo/data/reads/paired")

  sample_idx_tbl$REV_R1_paired[sample] <-
    all_fnFs[grep(pattern =  paste0("/",sample_idx_tbl$`Lib name R`[sample]),x = all_fnFs)] %>% str_replace(pattern = "raw",replacement = "analises/pipeline-modelo/data/reads/paired")

  sample_idx_tbl$REV_R2_paired[sample] <-
    all_fnRs[grep(pattern =  paste0("/",sample_idx_tbl$`Lib name R`[sample]),x = all_fnRs)] %>% str_replace(pattern = "raw",replacement = "analises/pipeline-modelo/data/reads/paired")

}


## Nomeando as reads com o nome das amostras ----
{
  names(sample_idx_tbl$FWD_R1) <- sample_idx_tbl$File_name
  names(sample_idx_tbl$FWD_R2) <- sample_idx_tbl$File_name
  names(sample_idx_tbl$REV_R1) <- sample_idx_tbl$File_name
  names(sample_idx_tbl$REV_R2) <- sample_idx_tbl$File_name
  names(sample_idx_tbl$FWD_R1_paired) <- sample_idx_tbl$File_name
  names(sample_idx_tbl$FWD_R2_paired) <- sample_idx_tbl$File_name
  names(sample_idx_tbl$REV_R1_paired) <- sample_idx_tbl$File_name
  names(sample_idx_tbl$REV_R2_paired) <- sample_idx_tbl$File_name
}

## Concatenações ----
{
  ## concatenando os endereços das reads R1 no objeto reads_fnFs
  reads_fnFs <- c(sample_idx_tbl$FWD_R1,sample_idx_tbl$REV_R1)

  ## concatenando os endereços das reads R2 no objeto reads_fnRs
  reads_fnRs <- c(sample_idx_tbl$FWD_R2,sample_idx_tbl$REV_R2)

  ## concatenando os endereços das reads R1 pareadas no objeto reads_fnFs_paired
  reads_fnFs_paired <- c(sample_idx_tbl$FWD_R1_paired,sample_idx_tbl$REV_R1_paired)

  ## concatenando os endereços das reads R1 pareadas no objeto reads_fnRs_paired
  reads_fnRs_paired <- c(sample_idx_tbl$FWD_R2_paired,sample_idx_tbl$REV_R2_paired)
}


## Conferindo os tamanhos dos vetores. Devem ter o mesmo tamanho ----
length(reads_fnFs)
length(reads_fnRs)
length(reads_fnFs_paired)
length(reads_fnRs_paired)


## Conferindo os nomes dos vetores. Devem ter os mesmos nomes, neste caso, o nome das amostras ----
names(reads_fnFs)
names(reads_fnRs)
names(reads_fnFs_paired)
names(reads_fnRs_paired)



## Verificar se cada arquivo com o mesmo índice possui o mesmo nome relacionado à amostra correspondente (1:24) ----
reads_fnFs[20]
reads_fnRs[20]
reads_fnFs_paired[20]
reads_fnRs_paired[20]
```

## DADA2

### Filtragem e o repareamento das reads com DADA2

Etapa prévia de filtragem e repareamento das reads com o **DADA2**.

O comando **fastqPairedFilter()** do pacote **DADA2** será utilizado a
partir dos seguintes parâmetros:

-   maxN = c(0,0): descarte das reads com N's

-   maxEE = c(2,2): remoção das reads com taxa de erro maior que 2

-   matchIDs = True: remoção das reads não-pareadas

-   isPhiX = True: descarte das reads de PhiX

Outras flags:

-   'compress = True' irá determinar que os arquivos de saída serão
    .gzip

-   'verbose = TRUE' determina que mensagens de erro irão aparecer

```{r, eval=FALSE,echo=TRUE}

## Filtrando as reads demultiplexadas ----

for (lib in 1:length(reads_fnFs)) {

  print(paste0("Working on file ",lib," of ",length(reads_fnFs),": ",names(reads_fnFs[lib])))

    dada2::fastqPairedFilter(fn = c(reads_fnFs[lib],reads_fnRs[lib]),
                           fout = c(reads_fnFs_paired[lib],reads_fnRs_paired[lib]),
                           maxN = c(0,0),
                           maxEE = c(2,2),
                           matchIDs = TRUE,
                           rm.phix = TRUE,
                           compress = TRUE,
                           verbose = TRUE)
}


## Preste atenção aos avisos, eles irão indicar quais tags estão com reads vazias ou ausentes
```

### Definindo o nível das amostras

Definindo a ordem que o nome das amostras irão aparecer nos gráficos.

```{r, eval=FALSE,echo=TRUE}

## Definindo os níveis das amostras ----
{
  sample_idx_tbl$File_name %>% unique() %>% sort() %>% paste0(collapse = '",\n"') %>% cat()

sample_levels <- c(
"L1_nov_dec_20_mi",
"L1_nov_dec_20_neo",
"L2_nov20",
"L2_dez20",
"L1_out21",
"L2_out21",
"L3_out21",
"L4_out21",
"L1_nov21",
"L2_nov21",
"L3_nov21",
"L4_nov21"
)
}
```

### Trabalhando com as *reads* limpas

Agora que as reads estão limpas e repareadas, podemos iniciar o
*workflow* do **DADA2**. A primeira etapa é aprender as taxas de erro
específicas por read, que irão orientar a dereplicação. A dereplicação é
o processo de agrupar as ASVs como sequências únicas.

```{r, echo=TRUE, eval=FALSE}

## Checar a qualidade das reads pareadas
dada2::plotQualityProfile(reads_fnFs_paired[1:2])

## Se a tolerância aos erros for muito baixa, (ie. -e 0), alguns arquivos deixarão de existir ----

file.exists(sample_idx_tbl$FWD_R1_paired)
file.exists(sample_idx_tbl$REV_R1_paired)
file.exists(sample_idx_tbl$FWD_R2_paired)
file.exists(sample_idx_tbl$REV_R2_paired)


## Verificando quais amostras possuem arquivos faltando ----
sample_idx_tbl$FWD_R1_paired[!file.exists(sample_idx_tbl$FWD_R1_paired)]
sample_idx_tbl$REV_R1_paired[!file.exists(sample_idx_tbl$REV_R1_paired)]
sample_idx_tbl$FWD_R2_paired[!file.exists(sample_idx_tbl$FWD_R2_paired)]
sample_idx_tbl$REV_R2_paired[!file.exists(sample_idx_tbl$REV_R2_paired)]


## Aprendendo as taxas de erro ----
{
  ## R1 
run_errF <- learnErrors(c(sample_idx_tbl$FWD_R1_paired[file.exists(sample_idx_tbl$FWD_R1_paired)],
                           sample_idx_tbl$REV_R1_paired[file.exists(sample_idx_tbl$REV_R1_paired)]),
                         multithread=TRUE,randomize = TRUE)
  ## R2
run_errR <- learnErrors(c(sample_idx_tbl$FWD_R2_paired[file.exists(sample_idx_tbl$FWD_R2_paired)],
                           sample_idx_tbl$REV_R2_paired[file.exists(sample_idx_tbl$REV_R2_paired)]),
                         multithread=TRUE,randomize = TRUE)
}
```

### Dereplicação: agrupando as sequências em ASVs

Nesta etapa cada bibilioteca é reduzida a suas sequências únicas e suas
contagens.

```{r, eval=FALSE}

## Dereplicação do DADA ----

## Reads FWD
{
  ## Reads FWD, sequências R1
  LGC_run_derep_FWD_R1 <- derepFastq(sample_idx_tbl$FWD_R1_paired[file.exists(sample_idx_tbl$FWD_R1_paired)], verbose=TRUE)
  
  ## Reads FWD, sequências R2
  LGC_run_derep_FWD_R2 <- derepFastq(sample_idx_tbl$FWD_R2_paired[file.exists(sample_idx_tbl$FWD_R2_paired)], verbose=TRUE)
}

## Reads REV
{
  ## Reads REV, sequências R1
  LGC_run_derep_REV_R1 <- derepFastq(sample_idx_tbl$REV_R1_paired[file.exists(sample_idx_tbl$REV_R1_paired)], verbose=TRUE)
  
  ## Reads REV, sequências R2
  LGC_run_derep_REV_R2 <- derepFastq(sample_idx_tbl$REV_R2_paired[file.exists(sample_idx_tbl$REV_R2_paired)], verbose=TRUE)
}
```

### Inferência das amostras

A função **dada()** recebe como entrada os sequências das *reads*
dereplicadas e retorna a o conjunto de ASVs que representam as
sequências biológicas reais. A inferência da composição de cada amostra
é realizada a partir da remoção das sequências que possuem erro.

```{r, eval=FALSE}

## Inferir as sequências a partir das reads dereplicadas ----

{  
  ## FWD R1
  LGC_run_FWD_dadaR1 <- dada(LGC_run_derep_FWD_R1, err=run_errF, multithread=TRUE)
  
  ## FWD R2
  LGC_run_FWD_dadaR2 <- dada(LGC_run_derep_FWD_R2, err=run_errR, multithread=TRUE)
  
  
  ## REV R1
  LGC_run_REV_dadaR1 <- dada(LGC_run_derep_REV_R1, err=run_errF, multithread=TRUE)
  
  ## REV R2
  LGC_run_REV_dadaR2 <- dada(LGC_run_derep_REV_R2, err=run_errR, multithread=TRUE)
}
```

### Unindo as sequências em ASVs

A função **mergePairs()** realiza a união (merge) das *reads* R1 e R2
que compõe as fitas FWD e REV, rejeitando os pares que não possuem
sobreposição suficiente de 20 bases (minOverlap = 20) ou que tenham
mismatches (maxMismatch = 0).

```{r, eval=FALSE}

## Conferir os objetos que estão armazenando as sequências que o dada2 inferiu ----
LGC_run_FWD_dadaR1
LGC_run_FWD_dadaR2
LGC_run_REV_dadaR1
LGC_run_REV_dadaR2

## Realizar a união dos sequências R1 e R2 da fita FWD e da fita REV

## FWD
{
  run_mergers_FWD <- mergePairs(dadaF = LGC_run_FWD_dadaR1,
                               derepF = LGC_run_derep_FWD_R1,
                               dadaR = LGC_run_FWD_dadaR2,
                               derepR = LGC_run_derep_FWD_R2,
                               minOverlap = 20,
                               maxMismatch = 0, ## pode ser alterado de 0 para 1 se tiverem pares faltando. Geralmente não é necessário em corridas de boa qualidade
                               returnReject = FALSE,
                               verbose=TRUE)
  }



## REV
{
  run_mergers_REV <- mergePairs(dadaF = LGC_run_REV_dadaR1,
                               derepF = LGC_run_derep_REV_R1,
                               dadaR = LGC_run_REV_dadaR2,
                               derepR = LGC_run_derep_REV_R2,
                               minOverlap = 20,
                               maxMismatch = 0, ## pode ser alterado de 0 para 1 se tiverem pares faltando. Geralmente não é necessário em corridas de boa qualidade
                               returnRejects = FALSE,
                               verbose=TRUE)
  }

length(run_mergers_FWD)
length(run_mergers_REV)
```

### Tabelas de abundâncias das ASVs (*Sequence tables*)

A função makeSequenceTable realiza a criação das Tabelas de abundâncias
das ASVs (*Sequence tables*). Porém, como existem 2 conjuntos de ASVs,
FWD e REV, suas Sequence tables devem ser unidas em uma só. Para tanto,
iremos realizar uma função **DADA2** customizada.

**Referência**: <https://github.com/benjjneb/dada2/issues/132>

```{r, echo=TRUE, eval=FALSE}

## Geração das Sequence tables ----

## Construção da matriz contendo as sequências e as amostras onde são encontradas
{
  run_seqtab_FWD <- makeSequenceTable(samples = run_mergers_FWD)
run_seqtab_REV <- makeSequenceTable(samples = run_mergers_REV)  ## makeSequenceTable gera uma matriz (análoga à uma OTU table) que possui cada ASV em uma coluna e os nomes das amostras em que elas estão presentes nas linhas
}

## Complemento reverso (rc) das sequências REV salvas na run_seqtab_REV
colnames(run_seqtab_REV) <- rc(colnames(run_seqtab_REV)) 

## Código da função sumSequenceTables para realizar a soma das seqtabs run_seqtab_FWD e run_seq_tab_REV
sumSequenceTables <- function(table1, table2, ..., orderBy = "abundance") {
  
  ## Combine passed tables into a list
  tables <- list(table1, table2)
  tables <- c(tables, list(...))
  
  ## Validate tables
  if(!(all(sapply(tables, dada2:::is.sequence.table)))) {
    stop("At least two valid sequence tables, and no invalid objects, are expected.")
  }
  sample.names <- rownames(tables[[1]])
  for(i in seq(2, length(tables))) {
    sample.names <- c(sample.names, rownames(tables[[i]]))
  }
  seqs <- unique(c(sapply(tables, colnames), recursive=TRUE))
  sams <- unique(sample.names)
  
  ## Make merged table
  rval <- matrix(0L, nrow=length(sams), ncol=length(seqs))
  rownames(rval) <- sams
  colnames(rval) <- seqs
  for(tab in tables) {
    rval[rownames(tab), colnames(tab)] <- rval[rownames(tab), colnames(tab)] + tab
  }
  
  ## Order columns
  if(!is.null(orderBy)) {
    if(orderBy == "abundance") {
      rval <- rval[,order(colSums(rval), decreasing=TRUE),drop=FALSE]
    } else if(orderBy == "nsamples") {
      rval <- rval[,order(colSums(rval>0), decreasing=TRUE),drop=FALSE]
    }
  }
  rval
}

## Merge das tabelas contendo as abundâncias das ASVs FWD e REV
mergers_seqtab <- sumSequenceTables(table1 = run_seqtab_FWD, 
                                    table2 = run_seqtab_REV)

## Dimensões da tabela contendo a abundância das ASVs FWD e REV
dim(run_seqtab_FWD)
dim(run_seqtab_REV)

## Dimensões da tabela do merge das ASVs FWD e REV
dim(mergers_seqtab) ## observar se está correto!!

## Geração das Sequence tables R1/R2 unmerged ----

## Seqtabs FWD
{
  FWD_dadaR1_seqtab <- makeSequenceTable(samples = LGC_run_FWD_dadaR1)
FWD_dadaR2_seqtab <- makeSequenceTable(samples = LGC_run_FWD_dadaR2)
}

## Seqtabs REV
{
  REV_dadaR1_seqtab <- makeSequenceTable(samples = LGC_run_REV_dadaR1)
REV_dadaR2_seqtab <- makeSequenceTable(samples = LGC_run_REV_dadaR2)
}

## Complemento reverso (rc) das sequências REV salvas na REV_dadaR1_seqtab e REV_dadaR2_seqtab
{
  colnames(REV_dadaR1_seqtab) <- dada2:::rc(colnames(REV_dadaR1_seqtab))
colnames(REV_dadaR2_seqtab) <- dada2:::rc(colnames(REV_dadaR2_seqtab))
}

## Merge das tabelas contendo as abundâncias das ASVs R1 e R2
{
  R1_seqtab <- sumSequenceTables(table1 = FWD_dadaR1_seqtab,
                               table2 = REV_dadaR1_seqtab)
R2_seqtab <- sumSequenceTables(table1 = FWD_dadaR2_seqtab,
                               table2 = REV_dadaR2_seqtab)
}

## Dimensões da tabela do merge das ASVs R1 e R2
dim(R1_seqtab)
dim(R2_seqtab) ## comparar se está correto!


## Checar objetos
dada2:::is.sequence.table(run_seqtab_FWD)
dada2:::is.sequence.table(run_seqtab_REV)
dada2:::is.sequence.table(R1_seqtab)
dada2:::is.sequence.table(R2_seqtab)
dada2:::is.sequence.table(mergers_seqtab)

#Checar dimensão dos objetos
length(run_seqtab_FWD) ## Seqtab das ASVs criadas após o merge e a dereplicação
length(run_seqtab_REV) ## Seqtab das ASVs criadas após o merge e a dereplicação
length(R1_seqtab) ## Reads sem realizar o merge
length(R2_seqtab) ## Reads sem realizar o merge
length(mergers_seqtab) # Seqtab após a soma das abundâncias de run_seqtab_FWD e run_seqtab_REV

## Checar a dimensão dos objetos
dim(run_seqtab_REV)
dim(run_seqtab_FWD)
dim(all_seqtab)
dim(mergers_seqtab)
str(all_seqtab)
str(mergers_seqtab)

sample_idx_tbl$File_name %>% unique()


# Avaliar a distribuição do comprimento das sequências ----
table(nchar(getSequences(run_seqtab_REV))) %>% plot()
table(nchar(getSequences(run_seqtab_FWD))) %>% plot()
table(nchar(getSequences(mergers_seqtab))) %>% plot()
```

### Remoção de *Bimeras*

A função **removeBimeraDenovo()** realiza a remoção de *bimeras* do
conjunto de ASVs. *Bimeras* são *chimeras* que possuem cada extremidade
relacionada a uma sequência parental distinta.

```{r, eval=FALSE}

## Remoção de  chimeras ----
mergers_seqtab.nochim <- removeBimeraDenovo(mergers_seqtab, method="consensus", multithread=TRUE, verbose=TRUE) ## Observar quantas bimeras foram identificadas nas sequências

## Cálculo da % de bimeras na amostra
{
  dim(mergers_seqtab.nochim)
ch_rat <- (sum(mergers_seqtab.nochim)/sum(mergers_seqtab)) 
ch_por <- format(round(100 * (1 - ch_rat), digits = 2), nsmall = 2)
sprintf("A porcentagem de bimeras é de %s%%", ch_por)
}

## Exibir a distribuição das ASVs por tamanho após remoção das bimeras
table(nchar(getSequences(mergers_seqtab.nochim)))
table(nchar(getSequences(mergers_seqtab.nochim))) %>% plot()

## Verificar se alguma amostra foi removida
rownames(mergers_seqtab.nochim)
rownames(mergers_seqtab)
```

### Quantificação de *reads* e ASVs restantes

```{r, eval=FALSE}

## Quantificar a proporção de reads através do pipeline ----
{
  colnames(sample_idx_tbl)

sample_idx_tbl <- sample_idx_tbl %>%
  pivot_longer(cols = c(FWD_R1, FWD_R2, REV_R1, REV_R2,
                        FWD_R1_paired, FWD_R2_paired,
                        REV_R1_paired, REV_R2_paired),
               names_to = "Stage", values_to = "Read file")


sample_idx_tbl %>% colnames()
}

## Contagem de reads nos arquivos brutos
raw_reads <- sample_idx_tbl %>% filter(Stage %in% c("FWD_R1",
                                                    "FWD_R2",
                                                    "REV_R1",
                                                    "REV_R2"))

getN <- function(x) sum(getUniques(x))

## Arquivos brutos
names(sample_idx_tbl$`Read file`) <- sample_idx_tbl$File_name 

raw_reads <- sample_idx_tbl %>% filter(Stage %in% c("FWD_R1","FWD_R2","REV_R1","REV_R2")) 

raw_reads_counts <- ShortRead::countFastq(dirPath = raw_reads$`Read file`) %>% as_tibble(rownames = "Read file")

raw_reads_counts <- raw_reads_counts %>% 
  left_join(y = (raw_reads %>%  mutate(`Read file` = basename(`Read file`)) 
                                                         ),by = "Read file") %>% 
  select(!c( `Read file`,nucleotides,scores))

## Contagem de reads brutas ----
tbl_raw_FWD_R1 <- raw_reads_counts[raw_reads_counts$Stage %in% c("FWD_R1"),] %>%
  select(`File_name`, records) %>%
  `colnames<-`(c("File_name", "Raw FWD_R1"))

tbl_raw_FWD_R2 <- raw_reads_counts[raw_reads_counts$Stage %in% c("FWD_R2"),] %>%
  select(`File_name`, records) %>%
  `colnames<-`(c("File_name", "Raw FWD_R2"))


tbl_raw_REV_R1 <- raw_reads_counts[raw_reads_counts$Stage %in% c("REV_R1"),] %>%
  select(`File_name`, records) %>%
  `colnames<-`(c("File_name", "Raw REV_R1"))

tbl_raw_REV_R2 <- raw_reads_counts[raw_reads_counts$Stage %in% c("REV_R2"),] %>%
  select(`File_name`, records) %>%
  `colnames<-`(c("File_name", "Raw REV_R2"))


## Geração de tabelas para as reads limpas (denoised reads)

## FWD
tbl_Denoised_FWD_R1 <- (sapply(LGC_run_FWD_dadaR1, getN) %>% as_tibble(rownames = "File_name")) %>% `colnames<-`(c("File_name", "Denoised FWD_R1"))

tbl_Denoised_FWD_R2 <- (sapply(LGC_run_FWD_dadaR2, getN) %>% as_tibble(rownames = "File_name")) %>% `colnames<-`(c("File_name", "Denoised FWD_R2"))

## REV
tbl_Denoised_REV_R1 <- (sapply(LGC_run_REV_dadaR1, getN) %>% as_tibble(rownames = "File_name")) %>% `colnames<-`(c("File_name", "Denoised REV_R1"))

tbl_Denoised_REV_R2 <- (sapply(LGC_run_REV_dadaR2, getN) %>% as_tibble(rownames = "File_name")) %>% `colnames<-`(c("File_name", "Denoised REV_R2"))

## Reads unidas FWD (merged reads)
tbl_Merged_FWD <- (sapply(run_mergers_FWD, getN) %>% as_tibble(rownames = "File_name")) %>% `colnames<-`(c("File_name", "Merged FWD"))

## Reads unidas REV (merged reads)
tbl_Merged_REV <- (sapply(run_mergers_REV, getN) %>% as_tibble(rownames = "File_name")) %>% `colnames<-`(c("File_name", "Merged REV"))

tbl_Merged <- (rowSums(mergers_seqtab) %>% as_tibble(rownames = "File_name")) %>% `colnames<-`(c("File_name", "Unique Merged"))

## Sem as bimeras
tbl_Non_chimeric <- (rowSums(mergers_seqtab.nochim) %>% as_tibble(rownames = "File_name")) %>% `colnames<-`(c("File_name", "Non-chimeric"))


## Combinar todas as contagens em um mesmo plot
all_track <- tbl_raw_FWD_R1 %>%
  left_join(tbl_raw_FWD_R2, by = "File_name") %>%
  left_join(tbl_raw_REV_R1, by = "File_name") %>%
  left_join(tbl_raw_REV_R2, by = "File_name") %>%
  left_join(tbl_Denoised_FWD_R1, by = "File_name") %>%
  left_join(tbl_Denoised_FWD_R2, by = "File_name") %>%
  left_join(tbl_Denoised_REV_R1, by = "File_name") %>%
  left_join(tbl_Denoised_REV_R2, by = "File_name") %>%
  left_join(tbl_Merged_FWD, by = "File_name") %>%
  left_join(tbl_Merged_REV, by = "File_name") %>%
  left_join(tbl_Merged, by = "File_name") %>%
  # left_join(tbl_R1_R2_merged,by = "File_name") %>% 
  left_join(tbl_Non_chimeric, by = "File_name") %>% 
  # left_join(tbl_Non_chi_R1, by = "File_name") %>% 
  # left_join(tbl_Non_chi_R2, by = "File_name") %>% 
  left_join(unique(primers_n_samples[c("Primer","File_name")]),by = "File_name") 


colnames(all_track) %>%  paste0(collapse = '",\n"') %>% cat


all_track <- all_track %>% select(c("File_name", "Primer",
                                    "Raw FWD_R1", "Raw FWD_R2", 
                                    "Raw REV_R1", "Raw REV_R2",
                                    "Denoised FWD_R1", "Denoised FWD_R2",
                                    "Denoised REV_R1", "Denoised REV_R2", 
                                    "Merged FWD", "Merged REV",
                                    "Unique Merged",
                                    # "R1 + R2 + merged non-chimeric",
                                    # "Non-chi_R1",
                                    # "Non-chi_R2",
                                    "Non-chimeric"))

## Salvar tabelas de contagens

writexl::write_xlsx(x = all_track,
                    path = paste0(results_path,"/",prjct_rad,"-reads_and_seqs_counts-",Sys.Date(),".xlsx"),
                    col_names = TRUE,format_headers = TRUE)

track_tbl <- bind_rows(all_track) 

## Plotar as proporções de reads ao longo do pipeline ----
options(scipen = 5)


## Definir as cores dos próximos plots ----

## colors 
scales::show_col(viridis::viridis(n = 5))
scales::show_col(viridis::viridis(n = 5))
# inferno5 <- viridis::inferno(n = 10)[5:9]
inferno8 <- viridis::inferno(n = 8)
turbo10 <- viridis::turbo(n = 10)

colnames(track_tbl)

## Transformar o objeto tibble para o formato longo, melhor para o ggplot2
  track_tbl <- track_tbl %>%
  gather(key = "Stage",
        value = "Read counts",
        "Raw FWD_R1", "Raw FWD_R2", 
        "Raw REV_R1", "Raw REV_R2", 
        "Denoised FWD_R1", "Denoised FWD_R2", 
        "Denoised REV_R1", "Denoised REV_R2", 
        "Merged FWD", "Merged REV",
        "Unique Merged",
        # "R1 + R2 + merged non-chimeric", 
        # "Non-chi_R1",
        # "Non-chi_R2",
        "Non-chimeric") %>%
  mutate(Stage = factor(Stage, levels = c(
    # "R1 + R2 + merged non-chimeric",
                                          "Non-chimeric",
                                          # "Non-chi_R1",
                                          # "Non-chi_R2",
                                          "Unique Merged", 
                                          "Merged FWD", "Merged REV", 
                                          "Denoised REV_R2", "Denoised REV_R1",
                                          "Denoised FWD_R2", "Denoised FWD_R1",
                                          "Raw REV_R2","Raw REV_R1", "Raw FWD_R2","Raw FWD_R1"))) 

    options(scipen = 22)
    
## Plotar a razão reads/abundância de cada amostra
      track_plot <- track_tbl %>% 
    mutate(File_name = factor(File_name, levels = sample_levels)) %>%
      ggplot(aes(y = Stage,x = `Read counts`, 
                 fill = Stage,
                 group = File_name
                 )) +
      geom_bar(stat="identity") +
      geom_hline(yintercept = 300000, col = 1, linetype = 2) +
    # scale_fill_viridis(discrete = TRUE,option = "viridis",alpha = 0.75,) +
    # scale_fill_brewer(palette = "Set2")+
    scale_fill_manual(                     
      values = alpha(colour = inferno8[c(2,3,4,4,5,5,5,5,6,6,6,6)],
                     
                                                          alpha =  0.75)) +
    labs(title = paste0(prjct_rad," - eDNA & Ovos e Larvas - Análise: ",Sys.Date()),
         subtitle = "Número de reads por biblioteca e etapa do processamento",
         x = "Número de reads",
         y = "Etapas de processamento dos dados")+
      # xlab(label = "Número de reads")+
      # ylab(label = "Etapas de processamento dos dados") +
      # ggtitle(label = "Ecomol 1st run",
              # subtitle = "Número de reads por por biblioteca e etapa do processamento") +
      facet_wrap(~File_name,ncol = ) +
    coord_fixed(ratio = 25000) +
      theme_bw(base_size = 8) +
    theme(axis.text.x = element_text(angle = 90,hjust = 0.0001,vjust = -0.00000000001,face = "bold")) +
    theme(legend.position = "bottom") +
    theme(axis.title = ggtext::element_markdown())

track_plot 

ggsave(file = paste0(figs_path,"/",prjct_rad,"track_samples.pdf"),
     plot = track_plot,
     device = "pdf",
     width = 55,
     height = 60,
     units = "cm",
     dpi = 600)


track_tbl$`Full name` %>% unique()

## O gráfico com a contagem de reads em cada etapa está disponível em: 
## /home/gabriel/projetos/peixes-eDNA/pipeline-modelo/results/figs/
```

## Classificação taxonômica das ASVs

Nesta etapa as ASVs identificadas pelo **DADA2**, juntamente com todas
as bibliotecas de cada primers, serão associados (ou não) às sequências
do 12S Sequences Database. O **DADA2** possui duas estratégias para
realizar a identificação das ASVs: em primeiro lugar,
**assingSpecies()** irá identificar as correspondências perfeitas entre
os *amplicons* e as sequências do 12S DataBase. A segunda abordagem,
**assignTaxonomy()**, irá usar um algorítmo Bayesiano Classificador
Ingênuo (NBC) RDP (Wang, 2007) com replicatas de *k-mers* tamanho 8 e
*bootstrap* de 100 para associar as ASVs às sequências do banco de
referências. Nesta abordagem, a classificação taxonômica é proporcional
à similaridade das sequências.

[**Atenção!** Prestar atenção à versão do Banco de Sequências de
referência]{.ul}

### Classificação taxonômica exata

```{r, eval=FALSE}

## Classificação taxonômica exata das ASVs ----
mergers_sps <- dada2::assignSpecies(seqs = mergers_seqtab.nochim, allowMultiple = 10,
                                refFasta =  "/home/heron/prjcts/fish_eDNA/data/refs/db/LGC/fev22/DB/LGC12Sdb-fev22-dada_SPs_fullDB_order.fasta",
                                tryRC=TRUE,
                                n = 20000,
                                verbose = TRUE) 
## Atenção! Observar quantas ASVs foram identificadas ao nível de espécie!

  
)

## Vizualizar a tabela contendo as ASVs identificadas ao nível de espécie e sua classificação taxonômica
View(mergers_sps)

mergers_csv_sp <- mergers_sps %>% as_tibble() %>% mutate(OTU = rownames(mergers_sps))

all_csv_sp <- bind_rows(mergers_csv_sp)

## Tabela final da classificação exata
View(all_csv_sp)
```

### Classificação taxonômica NBC RDP

```{r, eval=FALSE}

## Classificação taxonômica NBC RDP ----
mergers_taxa <- dada2::assignTaxonomy(seqs = mergers_seqtab.nochim,
                                  refFasta =  "/home/heron/prjcts/fish_eDNA/data/refs/db/LGC/fev22/DB/LGC12Sdb-fev22-dada_tax_fullDB_order.fasta",
                                  multithread=TRUE, tryRC=TRUE,taxLevels = c("Kingdom","Phylum","Class","Order","Family", "Genus", "Species","Specimen","Basin"),
                           outputBootstraps = TRUE, verbose = TRUE )

colnames(all_csv_sp) <- c("exact Genus", "exact Species", "ASV")


mergers_csv_taxa <- mergers_taxa$tax %>% as_tibble() %>% mutate(ASV = rownames(mergers_taxa))

## Tabela fial da classificação taxonômica com o algorítmo NBC RDP
View(mergers_csv_taxa)

mergers_csv_taxa$Species %>% unique()
```

## Phyloseq

Nesta etapa as ASVs associadas às suas respectivas classificações
taxonômicas atribuídas pelo **DADA2** e suas respectivas contagens por
amostra serão combinadas usando o pacote **Phyloseq**.

### Gerando a tabela de metadados por amostra

Aqui, os metadados dos experimentos serão associados a cada amostra.

```{r, eval=FALSE}

## Criação da tabela de amostras ----
colnames(primers_n_samples)
colnames(primers_n_samples) %>% paste0(collapse = '",\n"') %>% cat
unique(sample_idx_tbl)


samdf <- primers_n_samples[,c(
  "Run",
"Coleta",
"Sample Name",
"File_name",
"Type",
"Point",
"Filter",
"Num replicates",
"Obs",
"Primer",
# "Tag pairs",
# "Tag FWD",
# "Tag REV",
"Control"
                              )] 
samdf <- samdf %>% as.data.frame()
  rownames(samdf) <- samdf$File_name
```

A tabela de metadados das amostras deve ser customizada para cada
experimento.

### Interpretação dos dados no Phyloseq

O Phyloseq é utilizado para reunir em um mesmo objeto os datasets
otu_table, sample_data e tax_table. Ele gera um objeto (neste caso o
mergers_ps) que é de dificil manipulacao, e por isso depois ele sera
'derretido' em um objeto mais manipulavel. Este pacote foi incluido no
pipeline por já ser usado associado com o dada2

```{r, eval=FALSE}

## Interpretação de dados no Phyloseq ----
mergers_ps <- phyloseq::phyloseq(phyloseq::otu_table(mergers_seqtab.nochim, taxa_are_rows = FALSE),
                                 phyloseq::sample_data(samdf),
                                 phyloseq::tax_table(mergers_taxa$tax))

View(mergers_seqtab.nochim)
View(samdf)
View(mergers_taxa$tax)
```

### Merge and Flex Phyloseq results

Diferentes gráficos podem ser gerados, juntos ou separados, para todos
os primers/bibliotecas e níveis taxonômicos

```{r, eval=FALSE}

## "Derretendo" o objeto phyloseq em uma tabela
mergers_ps_tbl <- psmelt(mergers_ps) %>% as_tibble() %>% mutate(Read_origin = "merged") %>%  
  filter(Abundance >=1)
  mergers_ps_tbl$OTU %>% unique() %>% length()
  R1_ps_tbl$OTU %>% unique() %>% length()
  R2_ps_tbl$OTU %>% unique() %>% length()

## Combinando todas tabelas ps de todas entradas de ASVs
{
 all_ps_tbl <- bind_rows(mergers_ps_tbl)
all_ps_tbl$OTU %>% unique() %>% length()
colnames(all_ps_tbl)[colnames(all_ps_tbl) == "OTU"] <- "ASV"
unique(all_ps_tbl$ASV)
all_ps_tbl$File_name %>%  unique()
all_ps_tbl$File_name <- factor(all_ps_tbl$File_name, levels = sample_levels)
}

## Concatenando a tabela de identificações exatas
all_ps_tbl <- left_join(by = "ASV",x=all_ps_tbl,y= all_csv_sp)
```

## BLAST

### Uso do BLASTn para identificar as ASVs restantes

O **BLASTn** é usado para classificar as ASVs que não foram
identificadas pelo **DADA2** a partir do nosso Banco de Sequências 12S
rDNA. O **BLASTn** utiliza um banco de dados chamado **GenBank**, que
contém milhões de sequências de DNA anotadas e disponibilizadas
publicamente.

### 1. Preparação

```{r, eval=FALSE}

asvs_blast <- all_ps_tbl$ASV %>% unique() %>% as.character()

class(asvs_blast)

## Carregar funções do BLASTn ----
{
## Execução de comandos SHELL (Ref. Lucio RQ)
shell_exec <- function(cmd, .envir = parent.frame()) {
  if (!requireNamespace("processx", quietly = TRUE)) {
    rlang::abort(message = "Package `processx` package is not installed.")
  }
  cmd_res <- processx::run(
    command = "bash",
    args = c("-c", glue::glue(cmd, .envir = .envir)), echo_cmd = FALSE
  )
  return(cmd_res)
}

## Função para obter os nomes FASTA a partir do Banco de Sequências baseado nos subjectIDs
get_fasta_header <- function(id, db_path = "/data/databases/nt/nt") {
  command <- "blastdbcmd -db {db_path} -entry {id} -outfmt %t"
  result <- shell_exec(cmd = command)
  return(result$stdout)
}
```

### 2. BLASTn para cada ASV

```{r, eval=FALSE, echo=TRUE}

## Função para realizar o BLASTn para cada ASV 
run_blastn <- function(asv, db_path = "/data/databases/nt/nt", num_alignments = 3, num_thread = 40) {
  # blast_cmd <- "echo -e '>seq1\n{asv}' | blastn -db {db_path} -outfmt 6 -perc_identity 95 -qcov_hsp_perc 95 -num_threads {as.character(num_thread)} -num_alignments {as.character(num_alignments)}"
  # blast_cmd <- "echo -e '>seq1\n{asv}' | blastn -db {db_path} -outfmt 6 -max_hsps 1 -perc_identity 95 -qcov_hsp_perc 95 -num_threads {as.character(num_thread)} -num_alignments {as.character(num_alignments)}"
  #TODO implement qcohsp on results
  blast_cmd <- "echo -e '>seq1\n{asv}' | blastn -db {db_path} -outfmt \"6 std qcovhsp\" -max_hsps 1 -perc_identity 95 -qcov_hsp_perc 95 -num_threads {as.character(num_thread)} -num_alignments {as.character(num_alignments)}"
  blast_res <- shell_exec(cmd = blast_cmd)
  return(blast_res)
}

##
get_blastn_results <- function(asv, num_thread = 40) {
  blast_res <- run_blastn(asv, num_thread = num_thread)
  if (blast_res$status != 0) {
    rlang::abort(message = "Blast has not run correctly.")
  }
  `%>%` <- dplyr::`%>%`

  if (blast_res$stdout == "") {
    # rlang::inform(glue::glue("Sequence {asv} not found."))
    df_to_return <- tibble::tibble(`ASV` = asv)
    return(df_to_return)
  }
  blast_table <- blast_res$stdout %>%
    readr::read_delim(delim = "\t",
                      col_names = c("query","subject","indentity","length","mismatches","gaps",
                                    "query start","query end","subject start","subject end",
                                    "e-value","bitscore","qcovhsp"
                                    ),
                      trim_ws = TRUE, comment = "#"
    )

  blast_table$`subject header` <- purrr::map_chr(blast_table$subject, get_fasta_header)
  blast_table <- dplyr::relocate(blast_table, `subject header`)
  blast_table <- tibble::rowid_to_column(blast_table, var = "res")

  blast_table <- tidyr::pivot_wider(blast_table, id_cols = subject,  names_from = res, values_from = seq_len(ncol(blast_table)), names_glue = "{res}_{.value}")

  blast_table <- blast_table %>%
    dplyr::mutate(`ASV` = asv) %>%
    dplyr::relocate(starts_with("3_")) %>%
    dplyr::relocate(starts_with("2_")) %>%
    dplyr::relocate(starts_with("1_")) %>%
    dplyr::relocate(`ASV`)
  return(blast_table)
}
}

# Versões paralelas
cores_to_be_used <- future::availableCores() - 20 # Usar todos os cores -2 = 78 ## ALTERAR ISSO AQUI DEPOIS DE VER COMO FUNCIONA, O ORIGINAL ERA 2

cores_to_be_used

future::plan(future::multisession(workers = cores_to_be_used))


blast_res <- furrr::future_map_dfr(asvs_blast, get_blastn_results, num_thread = 1, .options = furrr::furrr_options(seed = NULL))

```

### 3. Resultados

```{r, eval=FALSE, echo=TRUE}

## Salvando os resultados do BLASTn em um arquivo ----
readr::write_csv(blast_res, paste0(results_path,"asv_blastn_res_3hits_",Sys.Date(),".csv"),append = FALSE)

base::save.image(paste0(analysis_path,"/",prjct_rad,"-env_",Sys.Date(),"_blast.RData"))

nrow(blast_res)
dim(blast_res)

blast_res <- blast_res %>%  filter(`1_res` == 1 ) #remover o que não deu nada

str(blast_res)

class(asvs_blast)

## Unir a tabela de ASVs com as IDs atribuídas pelo BLASTn
all_ps_tbl_blast <- left_join(x = all_ps_tbl,y = blast_res,by = "ASV")

all_ps_tbl_blast <- all_ps_tbl_blast %>% 
  mutate(File_name = factor(File_name,levels = sample_levels),
         # Read_origin = factor(Read_origin,levels = c("merged","R1","R2")),
         Primer = factor(Primer,levels = c("NeoFish","MiFish")),
         ASV = factor(ASV),
         Sample = factor(Sample),
         Run = factor(Run),
         Group = factor(Group),
         Expedition = factor(Expedition),
         Coleta = factor(Coleta),
         Sample.Name = factor(Sample.Name),
         Type = factor(Type),
         Point = factor(Point),
         Sub.point = factor(Sub.point),
         Depth = factor(Depth),
         Num.replicates = factor(Num.replicates),
         Obs = factor(Obs),
         Tag.pairs = factor(Tag.pairs),
         Tag.FWD = factor(Tag.FWD),
         Tag.REV = factor(Tag.REV),
         Control = factor(Control)
         # ,
         # Kingdom = factor(Kingdom),
         # Phylum = factor(Phylum),
         # Class = factor(Class),
         # Order = factor(Order),
         # Family = factor(Family),
         # Genus = factor(Genus),
         # Species = factor(Species),
         # Specimen = factor(Specimen),
         # Basin = factor(Basin),
         # `exact Genus` = factor(`exact Genus`),
         # `exact Species` = factor(`exact Species`)
         )

colnames(all_ps_tbl_blast)
```

### 4. Cálculo das abundâncias

```{r, eval=FALSE, echo=TRUE}

all_ps_tbl_blast <- all_ps_tbl_blast %>%
  mutate("Relative abundance to all samples" = 0,
         "Relative abundance on sample" = 0,
         "Sample total abundance" = 0)

abd_total <- sum(all_ps_tbl_blast$Abundance)



all_ps_tbl_blast <- all_ps_tbl_blast %>%
  dplyr::group_by(File_name,Read_origin) %>%        #now the abundance on sample is for merged/R1/R2 separetely
  mutate("Sample total abundance" = sum(Abundance),
         "Relative abundance to all samples" = Abundance/abd_total*100, ## exibir em porcentagem
         "Relative abundance on sample" = Abundance/`Sample total abundance`*100) ## exibir em porcentagem
%>%
  ungroup()

paste0(colnames(all_ps_tbl_blast),"\n") %>%  cat()

all_ps_tbl_blast %>% 
    # filter(Read_origin %in% c("merged")) %>% 
  select(c("Read_origin","File_name","Relative abundance on sample","Sample total abundance")) %>% View()
```

## FINAL id

```{r,echo=TRUE, eval=FALSE}

#all_ps_tbl_blast_bckp2 <- all_ps_tbl_blast
#all_ps_tbl_blast<- all_ps_tbl_blast_bckp2 


all_ps_tbl_blast <- all_ps_tbl_blast %>%
  mutate(`exact GenSp` = paste(`exact Genus`,`exact Species`,sep=" "))


all_ps_tbl_blast <- all_ps_tbl_blast %>%
  mutate("final ID" = if_else((`exact Species` %in% c(NA,"NA", "NA NA")),
                              if_else((Species %in% c(NA,"NA")),
                                      if_else(Genus %in% c(NA,"NA"),
                                              if_else(Family %in% c(NA,"NA"),
                                                    substr(as.character(`1_subject header`),1,30),
                                                    Family),
                                              Genus),
                                      Species),
                              as.character(`exact GenSp`)))



all_ps_tbl_blast <- all_ps_tbl_blast %>%
mutate(
  Kingdom = factor(Kingdom),
  Phylum = factor(Phylum),
  Class = factor(Class),
  Order = factor(Order),
  Family = factor(Family),
  Genus = factor(Genus),
  Species = factor(Species),
  Specimen = factor(Specimen),
  Basin = factor(Basin),
  `exact Genus` = factor(`exact Genus`),
  `exact Species` = factor(`exact Species`),
  `final ID` = factor(`final ID`)
  )

colnames(all_ps_tbl_blast)[colnames(all_ps_tbl_blast) == "ASV"] <- "ASV (Sequence)"
names(all_ps_tbl_blast)[which(names(all_ps_tbl_blast)== "ASV length")] <- "ASV Size (pb)"
```

### ASVs seqs

```{r,echo=TRUE, eval=FALSE}
#25 - recover all ASVs sequences to prepare fasta ----

#all ----
# giving our seq headers more manageable names (ASV_1, ASV_2...)
all_asv_seqs <- tibble("ASV (Sequence)" = unique(all_ps_tbl_blast$`ASV (Sequence)`))

all_asv_seqs <- all_asv_seqs %>%
  mutate("ASV length" = nchar(`ASV (Sequence)`),
  # mutate("ASV length" = nchar(unfactor(ASV)),
         "ASV header" = as.character(""))

all_asv_seqs <- all_asv_seqs[rev(base::order(all_asv_seqs$`ASV length`)),]

for (i in 1:nrow(all_asv_seqs)) {

  all_asv_seqs$`ASV header`[i] <- paste0(">ASV_", i, "_", all_asv_seqs$`ASV length`[i], "bp")

}


#combine ASV headers and all_ps_tbl
all_ps_tbl_blast <- dplyr::left_join(x = all_ps_tbl_blast,
                                     y = all_asv_seqs,
                                     by = "ASV (Sequence)" )


# making and writing out a fasta of our final ASV seqs with tax
for (asv in 1:nrow(all_asv_seqs)) {

  tax <- all_ps_tbl_blast %>%
      filter(`ASV (Sequence)` == all_asv_seqs$`ASV (Sequence)`[asv]) %>%
    # select("Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species", "Specimen") %>%
    select("Read_origin", "Family", "Genus", "Species", "final ID") %>%
    unique() %>%
    paste0(collapse = "|")

  all_asv_seqs$`ASV header`[asv] <- paste0(all_asv_seqs$`ASV header`[asv],"_",tax)

  # if (condition) {
  # fazer algum teste pra ver ser ta certo
  # }
}

#write fasta file with ASVs and Taxonomy
all_asv_fasta <- c(rbind(all_asv_seqs$`ASV header`, all_asv_seqs$`ASV (Sequence)`))

write(all_asv_fasta, paste0(results_path,"/",prjct_rad,"-all_ASVs_all_primers.fasta"))

all_ps_tbl_blast$Abundance %>% table() %>%  plot()
all_ps_tbl_blast$`Relative abundance on sample` %>% table() %>%  plot()
all_ps_tbl_blast$`ASV Size (pb)` %>% table() %>%  plot()
```

## SWARM

```{r,echo=TRUE, eval=FALSE}

asvs_abd <- all_ps_tbl_blast %>%
  group_by(`ASV (Sequence)`,`ASV header`) %>%
  mutate("ASV total abundance" = sum(Abundance)) %>%
  select(c(`ASV (Sequence)`,`ASV header`,`ASV total abundance`)) %>%
  unique() %>%
  mutate(`ASV header abd` = paste0(`ASV header`,"_",`ASV total abundance`))

#write fasta file with ASVs and Taxonomy
all_asv_fasta_abd <- c(rbind(asvs_abd$`ASV header abd`, asvs_abd$`ASV (Sequence)`))

write(all_asv_fasta_abd, paste0(results_path,"/",prjct_rad,"_abd.fasta"))

#run on bash
# heron@edna:/home/gabriel/projetos/peixes-eDNA/analises/LGC_run5_22dez21/results/swarm$ swarm -t 50 ../LGC_run5_22dez21_abd.fasta -s LGC_run5_22dez21-swarm.stats -o LGC_run5_22dez21-swarm.out -w LGC_run5_22dez21-representative_OTUs.fasta -i LGC_run5_22dez21-swarm.structure -f


swarm_clust <- list.files(path = swarm_path,
                          pattern = "swarm.out",
                          full.names = TRUE ) %>% 
  readr::read_lines()


asvs_abd <- asvs_abd %>% mutate("OTU"= 0)

# Função para mapear as OTUs (linhas do swarm) correspondentes a cada ASV 

find_otu <- function(ASV_header,clusters_swarm){
  
  ASV_OTU_tbl <- tibble::tibble(`ASV header abd` = ASV_header,
                                OTU = 0)
  
  for (line in 1:length(clusters_swarm)) {
    if (stringr::str_detect(string = clusters_swarm[line],
                            pattern = stringr::str_remove_all(string = ASV_header,
                                                              pattern = ">"))) {

      ASV_OTU_tbl$OTU <- line
      }
    }
              return(ASV_OTU_tbl)
} 

# Versões paralelas
cores_to_be_used <- future::availableCores() - 2 # Usar todos os cores -2 = 78
future::plan(future::multisession(workers = cores_to_be_used))


# 
# find_otu(ASV_header = asvs_abd$`ASV header abd`[30],
#          clusters_swarm = swarm_clust)



ASVs_and_OTUs <- furrr::future_map_dfr(asvs_abd$`ASV header`,
                                       clusters_swarm = swarm_clust,
                                       .f = find_otu,
                                       .options = furrr::furrr_options(seed = NULL))


ASVs_and_OTUs$`ASV header abd` %>% unique() %>% length()

ASVs_and_OTUs$OTU %>% unique() %>% length()





asvs_abd <- left_join(asvs_abd,
                      ASVs_and_OTUs,
                      by = "ASV header abd")



all_ps_tbl_blast <- left_join(all_ps_tbl_blast,asvs_abd[,c(1,5)])
# all_ps_tbl_blast <- left_join(select(all_ps_tbl_blast,-c("OTU")),asvs_abd[,c(1,5)])

all_ps_tbl_blast %>% select(`final ID`,OTU) %>% View() 
all_ps_tbl_blast %>% select(`final ID`,OTU,`ASV length`) %>% unique() %>% View() 



all_ps_tbl_blast %>% select(`final ID`,OTU) %>% select(OTU) %>% unique() 
all_ps_tbl_blast %>% select(`ASV (Sequence)`,`final ID`,OTU) %>% unique() 
```

### Reorganizar a tabela

```{r, eval=FALSE}

# all_ps_tbl_blast_bckp3 <- all_ps_tbl_blast

# 
# colnames(all_ps_tbl_blast) %>% paste0(collapse = '",\n"') %>% cat()

all_ps_tbl_blast <-
  all_ps_tbl_blast %>%
  select(c("Sample", "Abundance", "Run", "Group", "Expedition", "Coleta", "Sample.Name", "File_name", "OTU", "final ID", "Relative abundance to all samples", "Relative abundance on sample", "Sample total abundance", "Type", "Point", "Sub.point", "Depth", "Nu.replicates", "Obs", "Primer", "Quantidade.de.ovos.ou.larvas", "Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species", "Specimen", "Basin", "Read_origin",
"exact Genus", "exact Species", "exact GenSp",
# "1_res",
"1_subject header",
# "1_query",
"1_subject", "1_indentity", "1_length", "1_mismatches","1_gaps", "1_query start",
"1_query end", "1_subject start", "1_subject end", "1_e-value", "1_bitscore", "1_qcovhsp",
# "2_res",
"2_subject header",
# "2_query",
"2_subject", "2_indentity", "2_length", "2_mismatches", "2_gaps", "2_query start", "2_query end", "2_subject start", "2_subject end", "2_e-value", "2_bitscore", "2_qcovhsp",
# "3_res",
"3_subject header",
# "3_query",
"3_subject", "3_indentity", "3_length", "3_mismatches", "3_gaps", "3_query start",
"3_query end", "3_subject start", "3_subject end", "3_e-value", "3_bitscore", "3_qcovhsp", "Tag.pairs", "Tag.FWD", "Tag.REV", "Control", "ASV length", "ASV header",
# "ASV header abd",
"ASV (Sequence)"
  ))

# paste0(colnames(all_ps_tbl_blast),"\n") %>%  cat()
# names(all_ps_tbl_blast)[which(names(all_ps_tbl_blast)=="ASV")] <- "ASV (Sequence)"
names(all_ps_tbl_blast)[which(names(all_ps_tbl_blast)== "ASV length")] <- "Size (pb)"
```

### Identificação das ASVs presentes no controles Brancos/Negativos

```{r,echo=TRUE, eval=FALSE}

all_ps_tbl_blast <- all_ps_tbl_blast %>% mutate("Remove" = "Samples only")

sample_levels

all_ps_tbl_blast$Type %>% unique()


# all_ps_tbl_blast$Remove[(all_ps_tbl_blast$Type %in% c("POSITIVE CONTROL","FILTRATION BLANK","EXTRACTION BLANK","PCR BLANK"))] <- "Remove"

all_ps_tbl_blast %>% 
  group_by(Group) %>% 
  mutate()

# split data into two libraries ----
eDNA_ps_tbl_blast <-all_ps_tbl_blast[all_ps_tbl_blast$Group %in% c("eDNA"),]
ichth_ps_tbl_blast <-all_ps_tbl_blast[all_ps_tbl_blast$Group %in% c("Ichthyoplancton"),]


# Identify contamination based on respective controls----


eDNA_ps_tbl_blast$Remove[(eDNA_ps_tbl_blast$Type %in% c("Pos. Control", "Neg. control"))] <- "Controls"

ichth_ps_tbl_blast$Remove[(ichth_ps_tbl_blast$Type %in% c("Pos. Control", "Neg. control"))] <- "Controls"

# ----


#mark control replicates


# 
# eDNA_contam_ASVs <- eDNA_ps_tbl_blast$`ASV (Sequence)`[(eDNA_ps_tbl_blast$Remove %in% c("Controls"))] %>% unique() %>% as.tibble() %>% `colnames<-`("ASV (Sequence)")
# 
# Ichthyo_contam_ASVs <- ichth_ps_tbl_blast$`ASV (Sequence)`[(ichth_ps_tbl_blast$Remove %in% c("Controls"))] %>% unique() %>% as.tibble() %>% `colnames<-`("ASV (Sequence)")



eDNA_contam_ASVs <- eDNA_ps_tbl_blast %>%
  filter(Remove %in% "Controls") %>%
  group_by(`ASV (Sequence)`) %>%
  mutate("Max. ASV abd. in control" = max(`Relative abundance on sample`)) %>%
  ungroup() %>%
  select("ASV (Sequence)","Max. ASV abd. in control") %>%
  unique()



ichth_contam_ASVs <- ichth_ps_tbl_blast %>%
  filter(Remove %in% "Controls") %>%
  group_by(`ASV (Sequence)`) %>%
  mutate("Max. ASV abd. in control" = max(`Relative abundance on sample`)) %>%
  ungroup() %>%
  select("ASV (Sequence)","Max. ASV abd. in control") %>% unique()


all_ps_tbl_blast$`ASV (Sequence)`%>%  unique()
all_ps_tbl_blast$`ASV (Sequence)`[all_ps_tbl_blast$Remove != "Remove"] %>%  unique()
all_ps_tbl_blast$Remove != "Remove" %>%  unique()


for (line in 1:nrow(eDNA_ps_tbl_blast)) {
  for (asv in 1:nrow(eDNA_contam_ASVs)) {
    if (eDNA_ps_tbl_blast$`ASV (Sequence)`[line] == eDNA_contam_ASVs$`ASV (Sequence)`[asv] ) {
      if ((eDNA_ps_tbl_blast$`Relative abundance on sample`[line]) >= (1 *eDNA_contam_ASVs$`Max. ASV abd. in control`[asv]) ) {
        eDNA_ps_tbl_blast$`Abd. higher than in control` <- "Higher than 1x in control"
        eDNA_ps_tbl_blast$Remove <- "Probable contamintion"
      }else{
        eDNA_ps_tbl_blast$`Abd. higher than in control` <- "Lower than 1x in control"
        eDNA_ps_tbl_blast$Remove <- "Probable true detection"
      }
    }

  }

}


for (line in 1:nrow(ichth_ps_tbl_blast)) {
  for (asv in 1:nrow(ichth_contam_ASVs)) {
    if (ichth_ps_tbl_blast$`ASV (Sequence)`[line] == ichth_contam_ASVs$`ASV (Sequence)`[asv] ) {
      if ((ichth_ps_tbl_blast$`Relative abundance on sample`[line]) >= (1 *ichth_contam_ASVs$`Max. ASV abd. in control`[asv]) ) {
        ichth_ps_tbl_blast$`Abd. higher than in control` <- "Higher than 1x in control"
        ichth_ps_tbl_blast$Remove <- "Probable contamintion"
      }else{
        ichth_ps_tbl_blast$`Abd. higher than in control` <- "Lower than 1x in control"
        ichth_ps_tbl_blast$Remove <- "Probable true detection"
      }
    }

  }

}



eDNA_ps_tbl_blast %>% mutate()

ichth_ps_tbl_blast %>% mutate()


#backup
# all_ps_tbl_blast_old <- all_ps_tbl_blast

all_ps_tbl_blast <- bind_rows(eDNA_ps_tbl_blast, ichth_ps_tbl_blast)


all_ps_tbl_blast$`ASV (Sequence)` %>%  unique()
all_ps_tbl_blast$Sample %>%  table()
all_ps_tbl_blast$Remove %>%  table()
```

## Resultados

### Salvando as tabelas completas

```{r,echo=TRUE, eval=FALSE}


#order by abundance

smp_abd_ID <- all_ps_tbl_blast[rev(base::order(all_ps_tbl_blast$Abundance)),] %>%
  filter(`Abundance` > 0)

dim(smp_abd_ID)

writexl::write_xlsx(x = smp_abd_ID,
                    path = paste0(results_path,"/",prjct_rad,"-todas_info_da_analise_",Sys.Date(),".xlsx"),
                    col_names = TRUE,format_headers = TRUE)

#CAREFULL WITH THAT RAM EUGENE
(smp_abd_ID$`final ID`[1:200] %>% str_split(pattern = " ", n = 3,simplify = TRUE))[,c(1,2)]
smp_abd_ID$`final ID`[1:200] %>% str_extract(pattern = "^.[[:alnum:]]++.\\S.[[:alnum:]]++")



smp_abd_ID <- smp_abd_ID %>% mutate(
  Identification = str_extract(string = `final ID`,
                               pattern = "^.[[:alnum:]]++.\\S.[[:alnum:]]++"))



smp_abd_ID_summary <- smp_abd_ID %>%  
  filter(`Probable bacteria` == FALSE) %>%
  # filter(Abundance >= 20) %>% 
# group_by(Sample,`final ID`) %>% 
group_by(Type,Sample,`final ID`) %>% 
  summarize(
    Type = unique(Type),
    Primer = unique(Primer),
    Read_origin = unique(Read_origin),
    `Num ASVs` = length(unique(`ASV (Sequence)`)),
    `Num OTUs` = length(unique(`OTU`)),
            # `Species` = unique(Identification),
            `Total Abundance` = sum(Abundance)
            # ,
            # `Different ASVs` = (unique(Replicate))
            ) %>% 
  ungroup()
# %>% 
#   group_by(Sample.name) %>% 
#   mutate(`Replicates` = max())


            
            #summary with R1/R2 and merged
            smp_abd_ID_summary <- smp_abd_ID %>%  
              # filter(Remove == "Keep") %>% 
              # filter(Abundance >= 20) %>% 
            # group_by(Sample,`final ID`) %>% 
            group_by(Sample,Identification,Read_origin) %>% 
              summarize(Primer = unique(Primer),
                        `Num ASVs` = length(unique(`ASV (Sequence)`)),
                        # `Species` = unique(Identification),
                        `Total Abundance` = sum(Abundance),
                        `Minimum Identity` = min(`1_indentity`)
                        # ,
                        # `Different ASVs` = (unique(Replicate))
            )
# %>% 
#   group_by(Sample.name) %>% 
#   mutate(`Replicates` = max())


smp_abd_ID_summary <-  smp_abd_ID %>%  
  filter(Read_origin %in% c("R1")) %>% 
  # group_by(Sample) %>% 
  #  ungroup() %>% 
   # filter(Remove != "Keep") %>%
   # filter(Remove != "Keep") %>%
   # filter(Abundance >= 20) %>% 
   group_by(Sample,`final ID`) %>% 
   summarize(`Num ASVs` = length(unique(`ASV (Sequence)`)),
            `Species` = unique(`final ID`),
            `Total Abundance` = sum(Abundance)
            ) %>% 
  unique()


smp_abd_ID %>% 
  group_by(Primer,Read_origin) %>% 
  summarize(Primer = unique(Primer),
            Read_origin = unique(Read_origin),
            `Num amostras` = length(unique(File_name)),
            `Num ASVs encontradas` = length(unique(`ASV (Sequence)`)),
            `Num ASVs identificadas pelo BLASTn` = length(unique(`ASV (Sequence)`[!is.na(`final ID`)]))) %>% View()


writexl::write_xlsx(x = smp_abd_ID_summary,
                    path = paste0(results_path,"/",prjct_rad,"-summary_",Sys.Date(),".xlsx"),
                    col_names = TRUE,format_headers = TRUE)


smp_abd_ID %>% 
  # filter(Type %in% c("KLABIN")) %>% 
  group_by(Group,Type,Primer,Read_origin) %>% 
  summarize(Type = unique(Type),
            Primer = unique(Primer),
            Read_origin = unique(Read_origin),
            `Num amostras` = length(unique(File_name)),
            `Num ASVs encontradas` = length(unique(`ASV (Sequence)`)),
            `Num OTUs encontradas` = length(unique(`OTU`)),
            `Num ASVs identificadas pelo BLASTn` = length(unique(`ASV (Sequence)`[!is.na(`final ID`)])),
            `Proporção de ASVs identificadas` = (`Num ASVs identificadas pelo BLASTn`/`Num ASVs encontradas`*100),
            `Num ASVs prováveis bactérias` = length(unique(`ASV (Sequence)`[`Probable bacteria` == TRUE])),
            `Proporção de ASVs prováveis bactérias` = (`Num ASVs prováveis bactérias`/`Num ASVs encontradas`*100)
            ) %>%
  writexl::write_xlsx(
    path = paste0(results_path,"/",prjct_rad,"-primer_summary-",Sys.Date(),".xlsx"),
    col_names = TRUE,format_headers = TRUE)
```

```{r echo=FALSE,eval=FALSE}
# all_ps_tbl_blast_bckp5 <- all_ps_tbl_blast


#remove bacterias

all_ps_tbl_blast <- all_ps_tbl_blast %>% mutate("Probable bacteria" = if_else((stringr::str_detect(string = all_ps_tbl_blast$`final ID`,
                            pattern = "ncultured|bacter|proka|16S rRNA amplicon fragment from a soil sample")), TRUE, FALSE))
```

### Plots

```{r echo=TRUE,eval=FALSE}
## Obtencao dos dados
{
  raw_results_tbl <- read.csv("~/projetos/lagoa_ingleses/tabelas/raw/08mar22/v3_ed_li_22mar22-todas_info_da_analise_2022-04-08.csv",
                              sep = ";", header = T, dec = ",")
}

## Criacao da lista com os possiveis nomes atribuidos as ASVs
{
  raw_results_tbl %>% colnames()
  raw_results_tbl %>% colnames() %>% paste0(collapse = '",\n"') %>% cat()
}

## Agrupamento da ASVs que possuem os mesmos atributos abaixo
{
  grouped_by_ID_tbl <- raw_results_tbl %>%
    select(c(
      # ASV  Sequence ",
      "Sample",
      # "Abundance",
      # "Run",
      "Coleta",
      "Ano",
      "Sample.Name",
      "File_name",
      # "Type",
      "Point",
      # "Filter", # "Num replicates", # "Obs", # "Primer", # "Tag pairs", # "Tag FWD", # "Tag REV", # "Control", # "Kingdom", # "Phylum", # "Class", # "Order", # "Family", # "Genus", # "Species", # "Specimen", # "Basin", # "Read_origin", # "exact Genus",
      # "exact Species", # "X1_res", # "X1_subject header", # "X1_query", # "X1_subject", # "X1_indentity", # "X1_length", # "X1_mismatches", # "X1_gaps", # "X1_query start", # "X1_query end", # "X1_subject start", # "X1_subject end", # "X1_e value", # "X1_bitscore", # "X1_qcovhsp",
      # "X2_res", # "X2_subject header", # "X2_query", # "X2_subject", # "X2_indentity", # "X2_length", # "X2_mismatches", # "X2_gaps", # "X2_query start", # "X2_query end", # "X2_subject.start", # "X2_subject.end",# "X2_e.value", # "X2_bitscore", # "X2_qcovhsp", # "X3_res", # "X3_subject.header",
      # "X3_query", # "X3_subject", # "X3_indentity", # "X3_length", # "X3_mismatches", # "X3_gaps", # "X3_query.start", # "X3_query.end", # "X3_subject.start", # "X3_subject.end", # "X3_e.value", # "X3_bitscore", # "X3_qcovhsp", # "Relative.abundance.to.all.samples", 
      "Relative.abundance.on.sample", # "Sample total abundance", #"exact GenSp", #"final ID",
      "Curated.ID", # "Size  pb ", # "ASV header", # "Remove
    )) %>% group_by(Sample, `Curated.ID`, Coleta, Point , `Sample.Name`, File_name) %>%
    summarize(`Sample` = unique(Sample),
              `Curated.ID` = unique(`Curated.ID`),
              `Coleta` = unique(Coleta),
              `Ano` = unique(Ano),
              `Point` = unique(Point),
              `Sample.Name` = unique(`Sample.Name`),
              `File_name` = unique(File_name),
              `RRA` = sum(`Relative.abundance.on.sample`)) %>%
    ungroup()
}

## Trocando os nomes dos animais não-peixes
{
  grouped_by_ID_tbl$`Curated.ID`[grouped_by_ID_tbl$`Curated.ID` == "Bos taurus"] <- "Bos taurus (Boi)"
  grouped_by_ID_tbl$`Curated.ID`[grouped_by_ID_tbl$`Curated.ID` == "Canis familiaris"] <- "Canis familiaris (Cachorro doméstico)"
  grouped_by_ID_tbl$`Curated.ID`[grouped_by_ID_tbl$`Curated.ID` == "Cavia magna"] <- "Cavia magna (Preá)"
  grouped_by_ID_tbl$`Curated.ID`[grouped_by_ID_tbl$`Curated.ID` == "Hydrochaeris hydrochaeris"] <- "Hydrochaeris hydrochaeris (Capivara)"
  grouped_by_ID_tbl$`Curated.ID`[grouped_by_ID_tbl$`Curated.ID` == "Nannopterum brasilianus"] <- "Nannopterum brasilianus (Biguá)"
  grouped_by_ID_tbl$`Curated.ID`[grouped_by_ID_tbl$`Curated.ID` == "Progne chalybea"] <- "Progne chalybea (Andorinha-grande)"
  grouped_by_ID_tbl$`Curated.ID`[grouped_by_ID_tbl$`Curated.ID` == "Sus scrofa"] <- "Sus scrofa (Javali)"
}

## Organizar as especies
{
  grouped_by_ID_tbl$`Curated.ID` %>% unique()%>% sort() %>%  paste0(collapse = '",\n"') %>% cat()
  grouped_by_ID_tbl$Sample %>% unique()%>% sort() %>%  paste0(collapse = '",\n"') %>% cat()
}

## Organizar as ordem das especies usando fatores

## Com todas as identificacoes
{
  grouped_by_ID_tbl <- grouped_by_ID_tbl %>%
    mutate(Curated.ID = factor(Curated.ID,
                               levels = rev(c( #"",
                                 "Astyanax",
                                 "Astyanax fasciatus",
                                 "Astyanax lacustris",
                                 "Brycon orthotaenia",
                                 "Bryconamericus stramineus",
                                 "Characidae",
                                 "Characidium",
                                 "Cichlidae",
                                 "Colossoma macropomum",
                                 "Eigenmannia virescens",
                                 "Gymnotus carapo",
                                 "Hemigrammus gracilis",
                                 "Hemigrammus marginatus",
                                 "Hoplias",
                                 "Hoplias intermedius",
                                 "Hoplias malabaricus",
                                 "Hoplias sp",
                                 "Hypomasticus steindachneri",
                                 "Leporellus vittatus",
                                 "Leporinus piau",
                                 "Leporinus reinhardti",
                                 "Leporinus taeniatus",
                                 "Loricariidae",
                                 "Megaleporinus elongatus",
                                 "Megaleporinus garmani",
                                 "Moenkhausia costae",
                                 "Myleus micans",
                                 "Oreochromis niloticus",
                                 "Oreochromis niloticus ",
                                 "Orthospinus franciscensis",
                                 "Phalloceros sp",
                                 "Pimelodus",
                                 "Pimelodus fur",
                                 "Pimelodus maculatus",
                                 "Pimelodus pohli",
                                 "Planaltina myersi",
                                 "Prochilodus costatus",
                                 "Pseudoplatystoma corruscans",
                                 "Pygocentrus piraya",
                                 "Rhamdia quelen",
                                 "Salmo salar",
                                 "Serrasalmus brandtii",
                                 "Tilapia rendalli",
                                 "Wertheimeria maculata",
                                 #não-peixes
                                 "Homo sapiens",
                                 "Bos taurus (Boi)",
                                 "Canis familiaris (Cachorro doméstico)",
                                 "Cavia magna (Preá)",
                                 "Hydrochaeris hydrochaeris (Capivara)",
                                 "Nannopterum brasilianus (Biguá)",
                                 "Progne chalybea (Andorinha-grande)",
                                 "Sus scrofa (Javali)"
                               ))))
}

## Criacao do Tile Plot das amostras da Lagoa dos Ingleses sequenciadas nas corridas 2, 4 e 5

## Exibindo  por ponto, por mês
{
  grouped_by_ID_tbl %>%
    #transformando variaveis categoricas em fatores com niveis
    mutate(Coleta = factor(Coleta)) %>%
    mutate(Sample = factor(Sample,
                           levels = c("L1_nov_dec_20_mi", 
                                      "L1_nov_dec_20_neo", 
                                      "L2_dez20", 
                                      "L2_nov20",
                                      "L1_nov21", 
                                      "L1_out21", 
                                      "L2_nov21", 
                                      "L2_out21", 
                                      "L3_nov21", 
                                      "L3_out21", 
                                      "L4_nov21", 
                                      "L4_out21" ))) %>% 
    mutate(Coleta = factor(Coleta, 
                           levels = c("Nov_Dec/20",
                                      "Nov/20",
                                      "Dec/20",
                                      "out/21",
                                      "Nov/21"
                                      ))) %>%
    mutate(Point = factor(Point)) %>%
    mutate(File_name = factor(File_name)) %>%
    mutate(Coleta = factor(Coleta)) %>%
    
    #filtrando apenas o que vc quer mostras
    filter(!Curated.ID %in% c("Astyanax",
                              "Characidae",
                              "Characidium",
                              "Cichlidae",
                              "Hoplias",
                              "Loricariidae",
                              "Pimelodus",
                              "Salmo salar")) %>%
    
    #retirar os NA
    filter(!is.na(Curated.ID)) %>%
    
    #retirando as ASVs "espurias", com abundancia menor que 0.01%
    filter(RRA >=0.01) %>%
    
    #Tile plot
    ggplot(aes(y = Curated.ID,
               x = Point,
               fill = RRA,
               # col = Coleta,
    )) +
    geom_tile() +
    # scale_color_manual()+
    facet_grid(~Coleta,
               scales = "free_x",
               space = "free_x") +
    # facet_grid(~Point,
    #            scales = "free_x",
    #            space = "free_x") +
    #facet_grid(~Year,
    #            scales = "free_x",
    #            space = "free_x") +
    labs(fill='Relative Read\nAbundance (%)',
         x = "Amostras",
         y= "Espécies") +
    geom_hline(yintercept = c(6.5)) +
      viridis::scale_fill_viridis(
      alpha = 1,
      begin = 0.75,
      end = 0,
      direction = 1,
      discrete = FALSE,
      option = "D"
    )
    theme(text=element_text(size = 10, face = "bold"))
  
}

## Criacao do Tile Plot das amostras da Lagoa dos Ingleses sequenciadas nas corridas 2, 4 e 5
## Exibindo  por ano apenas
{
  grouped_by_ID_tbl %>%
    
    #transformando variaveis categoricas em fatores com niveis
    mutate(Coleta = factor(Coleta)) %>%
    mutate(Sample = factor(Sample,
                           levels = c("L1_nov_dec_20_mi", 
                                      "L1_nov_dec_20_neo", 
                                      "L2_dez20", 
                                      "L2_nov20",
                                      "L1_nov21", 
                                      "L1_out21", 
                                      "L2_nov21", 
                                      "L2_out21", 
                                      "L3_nov21", 
                                      "L3_out21", 
                                      "L4_nov21", 
                                      "L4_out21" 
                                      
                           ))) %>% mutate(Coleta = factor(Coleta,
                                                              levels = c("Nov_Dec/20",
                                                                         "Nov/20",
                                                                         "Dec/20",
                                                                         "out/21",
                                                                         "Nov/21"
                                                              ))) %>%
    mutate(Point = factor(Point)) %>%
    mutate(File_name = factor(File_name)) %>%
    mutate(Coleta = factor(Coleta)) %>%
    
    #filtrando apenas o que vc quer mostras
    # filter(!Curated.ID %in% c("Astyanax",
    #                           "Characidae",
    #                           "Characidium",
    #                           "Cichlidae",
    #                           "Hoplias",
    #                           "Loricariidae",
    #                           "Pimelodus",
    #                           "Salmo salar")) %>%
    
    ## Retirando as ASVs "espurias", com abundancia menor que 0.01
    # filter(RRA >=0.01) %>%
    
    ## Retirar os NA
    filter(!is.na(Curated.ID)) %>%
    
    ## Tile plot
    ggplot(aes(y = Curated.ID,
               x = Ano,
               fill = RRA,
               # col = Coleta,
    )) +
    scale_x_continuous(breaks = 0:2100) +
    
    # geom_tile() +
    geom_tile() +
    # scale_color_manual()+
    #facet_grid(~Coleta,
    #           scales = "free_x",
    #           space = "free_x") +
    # facet_grid(~Point,
    #            scales = "free_x",
    #            space = "free_x") +
    #facet_grid(~Year,
    #            scales = "free_x",
    #            space = "free_x") +
    labs(fill='Relative Read\nAbundance (%)',
         x = "Amostras",
         y= "Espécies") +
    geom_hline(yintercept = c(8.5)) +
    viridis::scale_fill_viridis(
      alpha = 1,
      begin = 0.75,
      end = 0,
      direction = 1,
      discrete = FALSE,
      option = "D"
    )
    theme(text=element_text(size = 10, face = "bold"))
```
